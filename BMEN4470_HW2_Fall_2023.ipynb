{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zXZy6DyLjl0S"
   },
   "source": [
    "# **BMEN 4470 - Deep Learning for Biomedical Signal Processing**\n",
    "# **Homework 2: Basics of Modeling Sequences and Temporal Convolution Networks**\n",
    "\n",
    "Due 11:59pm on October 16th, 2023\n",
    "\n",
    "Please include the names of people you've collaborated with here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "id": "9HrJJcnTitXl"
   },
   "outputs": [],
   "source": [
    "#%pip install bunch\n",
    "#Import\n",
    "import sys\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "import scipy.io as sio\n",
    "from bunch import Bunch\n",
    "import torch.optim as optim\n",
    "from pandas import DataFrame\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.nn.utils import weight_norm\n",
    "from os.path import dirname, join as pjoin\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from statsmodels.tsa.arima_process import ArmaProcess\n",
    "from sklearn.metrics import roc_auc_score, precision_score, recall_score, accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LXlQA6-13xlb"
   },
   "source": [
    "**Problem 1: ARMA Model** In this question, you need to apply three different Autoregressive Moving Average Model (ARMA) to the given EKG signal. Compare the input and output signals and answering the following questions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "id": "AMe0T0yRDnhE"
   },
   "outputs": [],
   "source": [
    "# The EGK signal contains 100 data points\n",
    "EKG_sig = [41.985077,48.505554,17.868908,-17.902781,-25.676805,2.4074743,40.308300,50.589020,22.663403,-16.953960,-32.144905,-31.155527,-23.732307,6.5244060,44.417355,58.700817,60.295696,58.277584,51.427200,24.971256,18.454710,42.659885,28.012609,13.741727,14.357091,15.576780,11.927580,-14.444111,-25.185453,-19.081909,6.6035562,17.284658,17.544495,16.928368,-6.3161540,-14.990187,10.754345,21.378378,-3.1653652,-13.225622,15.521652,54.976814,64.869339,35.326790,-5.0551395,-18.008286,-15.629535,-11.389389,15.224802,52.611454,69.125626,71.429947,69.159081,63.929958,35.759071,-1.0452937,-12.551209,9.9472542,23.483477,29.982676,53.127209,38.299618,26.093702,25.454752,0.92754990,-9.3693218,19.490158,56.732342,40.379166,1.2176796,15.432666,53.198875,38.633102,1.4428799,12.662603,25.239782,24.835445,19.891754,-6.5586896,3.0409441,15.515370,13.531504,6.8291478,-15.963812,0.38266551,33.888634,14.340966,-27.637432,-37.876808,-12.587003,-1.4763706,-1.0964720,20.268143,5.8535609,-29.653194,-16.162226,18.147284,6.5141749,-4.5157704,13.640582]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zGOUQpZrDz0E"
   },
   "source": [
    "**1(a)** Moving Average model only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bFsJ_nSh6qSJ"
   },
   "outputs": [],
   "source": [
    "model_1 = ARIMA(EKG_sig, order=[0, 0, 1])# put the order here\n",
    "model_fit_1 = model_1.fit()\n",
    "residuals_1 = DataFrame(model_fit_1.resid)\n",
    "\n",
    "# line plot of the results\n",
    "plt.plot(EKG_sig, label='EKG')\n",
    "plt.plot(residuals_1, label='MA')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-rfJzLtkF4CU"
   },
   "source": [
    "**1(b)** Autoregressive model only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1A4YlSVsGMVj"
   },
   "outputs": [],
   "source": [
    "model_2 = ARIMA(EKG_sig, order=[1, 0, 0])# put the order here\n",
    "model_fit_2 = model_2.fit()\n",
    "residuals_2 = DataFrame(model_fit_2.resid)\n",
    "\n",
    "# line plot of the results\n",
    "plt.plot(EKG_sig, label='EKG')\n",
    "plt.plot(residuals_2, label='AR')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-LWbyVlFG1e_"
   },
   "source": [
    "**1(c)** ARMA model with p = 5, q = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oT5oyayMG1uX"
   },
   "outputs": [],
   "source": [
    "model_3 = ARIMA(EKG_sig, order=[5, 0, 1])# put the order here\n",
    "model_fit_3 = model_3.fit()\n",
    "residuals_3 = DataFrame(model_fit_3.resid)\n",
    "\n",
    "# line plot of the results\n",
    "plt.plot(EKG_sig, label='EKG')\n",
    "plt.plot(residuals_3, label='ARMA')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IB60UnHtIlvT"
   },
   "source": [
    "**1(d)** What are the differences between those three models? Which model works best for this EKG data? What are the pros and cons of each model? Please list at least one pro and con for each model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q1pBSaRJJbcp"
   },
   "source": [
    "Your Answers:\n",
    "- From Observation, the Moving Average Model was able to predict the same shape of the EKG data, showing that it's able to predict the relative amplitude of each data point. \n",
    "- The auto regressive model has a larger amplitude on the predictions, but shows significantly more lag when the sign changes for the data. \n",
    "- The ARMA model had the worst predictions of them all. There seems to be an onvercompensation of the lag for the predction. \n",
    "\n",
    "- The moving average model is best at minimizing or eliminating the errors for the data prediction. \n",
    "- The Auto Regressive model is better at matching trends. For example if the data is trending upwards or downwards, it's better at predicting the correct results. \n",
    "- the ARMA model was able to show both, but with the given parameters it might be overcompensating. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7BATDuHPsLlj"
   },
   "source": [
    "**Problem 2 Temporal Convolutional Networks** In this question, we will generate a dataset and build a simple TCN to train and test on the dataset. \n",
    "\n",
    "This dataset contains some data sequences: a pair of input sequences and an output. The first sequence within the input pair is composed of numbers randomly sampled from the range [0, 1]. The second input sequence within the input pair is composed of only integers: 0 or 1. This sequence must contain only two integer 1; the rest of the integers are all 0s. The output is the sum of the two values from the first input sequence corresponding to 1s in the second input sequence. \n",
    "\n",
    "For example, one input sequence pair can be [[0, 0.1, 0.2, 0.3, 0.4, 0.5], [0, 0, 0, 1, 1, 0]]. In this case, the final output value should be 0.3 + 0.4 = 0.7. In this example, the length of the data sequence is 6.\n",
    "\n",
    "For the detail of TCN, see the GitHub repo [here](https://github.com/locuslab/TCN/tree/master/TCN/adding_problem)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "id": "9_zBwJOusWnZ"
   },
   "outputs": [],
   "source": [
    "# data generator\n",
    "def data_generator(N, seq_length):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        seq_length: Length of the adding problem data\n",
    "        N: # of data in the set\n",
    "    \"\"\"\n",
    "    X_num = torch.rand([N, 1, seq_length])\n",
    "    X_mask = torch.zeros([N, 1, seq_length])\n",
    "    Y = torch.zeros([N, 1])\n",
    "    for i in tqdm(range(N)):\n",
    "        positions = np.random.choice(seq_length, size=2, replace=False)\n",
    "        X_mask[i, 0, positions[0]] = 1\n",
    "        X_mask[i, 0, positions[1]] = 1\n",
    "        Y[i,0] = X_num[i, 0, positions[0]] + X_num[i, 0, positions[1]]\n",
    "    X = torch.cat((X_num, X_mask), dim=1)\n",
    "    return Variable(X), Variable(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "id": "BmQsdvIHRxmA"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [00:00<00:00, 22908.88it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 20424.15it/s]\n"
     ]
    }
   ],
   "source": [
    "# generate data\n",
    "X_train, Y_train = data_generator(5000, 100)\n",
    "X_test, Y_test = data_generator(100, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "id": "Lcxbz3hs_o-S"
   },
   "outputs": [],
   "source": [
    "# define several classes for training.\n",
    "class TCN(nn.Module):\n",
    "    def __init__(self, input_size, output_size, num_channels, kernel_size, dropout):\n",
    "        super(TCN, self).__init__()\n",
    "        self.tcn = TemporalConvNet(input_size, num_channels, kernel_size=kernel_size, dropout=dropout)\n",
    "        self.linear = nn.Linear(num_channels[-1], output_size)\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        self.linear.weight.data.normal_(0, 0.01)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y1 = self.tcn(x)\n",
    "        return self.linear(y1[:, :, -1])\n",
    "\n",
    "\n",
    "class Chomp1d(nn.Module):\n",
    "    def __init__(self, chomp_size):\n",
    "        super(Chomp1d, self).__init__()\n",
    "        self.chomp_size = chomp_size\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x[:, :, :-self.chomp_size].contiguous()\n",
    "\n",
    "class TemporalConvNet(nn.Module):\n",
    "    def __init__(self, num_inputs, num_channels, kernel_size=2, dropout=0.2):\n",
    "        super(TemporalConvNet, self).__init__()\n",
    "        layers = []\n",
    "        num_levels = len(num_channels)\n",
    "        for i in range(num_levels):\n",
    "            dilation_size = 2 ** i\n",
    "            in_channels = num_inputs if i == 0 else num_channels[i-1]\n",
    "            out_channels = num_channels[i]\n",
    "            layers += [TemporalBlock(in_channels, out_channels, kernel_size, stride=1, dilation=dilation_size,\n",
    "                                     padding=(kernel_size-1) * dilation_size, dropout=dropout)]\n",
    "\n",
    "        self.network = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using mps device\n"
     ]
    }
   ],
   "source": [
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bf3AG8CWC40F"
   },
   "source": [
    "**2(a)** Build your TCN model below. Your model should contain two convolutional layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "id": "mN_krETLCwxY"
   },
   "outputs": [],
   "source": [
    "class TemporalBlock(nn.Module):\n",
    "    def __init__(self, n_inputs, n_outputs, kernel_size, stride, dilation, padding, dropout=0.2):\n",
    "        super(TemporalBlock, self).__init__()\n",
    "        # Your Code:\n",
    "        self.conv1 = weight_norm(nn.Conv1d(n_inputs, n_outputs, kernel_size,\n",
    "                                           stride=stride, padding=padding, dilation=dilation))\n",
    "        self.chomp1 = Chomp1d(padding)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "\n",
    "        self.conv2 = weight_norm(nn.Conv1d(n_outputs, n_outputs, kernel_size,\n",
    "                                           stride=stride, padding=padding, dilation=dilation))\n",
    "        self.chomp2 = Chomp1d(padding)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "\n",
    "        self.net = nn.Sequential(self.conv1, self.chomp1, self.relu1, self.dropout1,\n",
    "                                 self.conv2, self.chomp2, self.relu2, self.dropout2)\n",
    "        self.downsample = nn.Conv1d(n_inputs, n_outputs, 1) if n_inputs != n_outputs else None\n",
    "        self.relu = nn.ReLU()\n",
    "        self.init_weights()\n",
    "\n",
    "\n",
    "\n",
    "    def init_weights(self):\n",
    "        self.conv1.weight.data.normal_(0, 0.01)\n",
    "        self.conv2.weight.data.normal_(0, 0.01)\n",
    "        if self.downsample is not None:\n",
    "            self.downsample.weight.data.normal_(0, 0.01)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.net(x)\n",
    "        res = x if self.downsample is None else self.downsample(x)\n",
    "        return self.relu(out + res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "self.conv1 = weight_norm(nn.Conv2d(n_inputs, n_outputs, (1, kernel_size),\n",
    "                                           stride=stride, padding=padding, dilation=dilation))\n",
    "        self.pad = nn.ZeroPad2d((padding, 0, 0, 0))\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.conv2 = weight_norm(nn.Conv2d(n_outputs, n_outputs, (1, kernel_size),\n",
    "                                           stride=stride, padding=padding, dilation=dilation))\n",
    "        self.net = nn.Sequential(self.pad, self.conv1, self.relu, self.dropout,\n",
    "                                 self.pad, self.conv2, self.relu, self.dropout)\n",
    "        self.downsample = nn.Conv1d(\n",
    "            n_inputs, n_outputs, 1) if n_inputs != n_outputs else None\n",
    "        self.relu = nn.ReLU()\n",
    "        self.init_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qmBOwqphE32Z"
   },
   "source": [
    "**2(b)** Set and adjust the training parameters to make the Average Difference you got from your network smaller than 0.008. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {
    "id": "2CgQm0n87_AX"
   },
   "outputs": [],
   "source": [
    "# initialize training parameters.\n",
    "args = Bunch()\n",
    "# adjustable parameters\n",
    "args.epochs = 15# put values here\n",
    "args.ksize = 4# put values here\n",
    "args.batch_size = 2# put values here\n",
    "args.lr = 1e-4# put values here\n",
    "\n",
    "# fix parameters\n",
    "args.cuda = False\n",
    "args.dropout = False\n",
    "args.seq_len = 100\n",
    "args.clip = -1\n",
    "args.levels = 8\n",
    "args.log_interval = 100\n",
    "args.optim = 'Adam'\n",
    "args.nhid = 30\n",
    "args.seed = 112\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OSICsQXAEfqv"
   },
   "source": [
    "Train your model. You need to use GPU for training here. To start a GPU on your Colab, follow the instruction on the slides for this assignment. Please re-run the first block after you set up the GPU. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {
    "id": "PXkcUFk9sWaB"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch:  1 [   198/  5000 (4%)]\tLearning rate: 0.0001\tLoss: 0.872483\n",
      "Train Epoch:  1 [   398/  5000 (8%)]\tLearning rate: 0.0001\tLoss: 0.172663\n",
      "Train Epoch:  1 [   598/  5000 (12%)]\tLearning rate: 0.0001\tLoss: 0.177692\n",
      "Train Epoch:  1 [   798/  5000 (16%)]\tLearning rate: 0.0001\tLoss: 0.190298\n",
      "Train Epoch:  1 [   998/  5000 (20%)]\tLearning rate: 0.0001\tLoss: 0.166759\n",
      "Train Epoch:  1 [  1198/  5000 (24%)]\tLearning rate: 0.0001\tLoss: 0.173796\n",
      "Train Epoch:  1 [  1398/  5000 (28%)]\tLearning rate: 0.0001\tLoss: 0.199630\n",
      "Train Epoch:  1 [  1598/  5000 (32%)]\tLearning rate: 0.0001\tLoss: 0.177069\n",
      "Train Epoch:  1 [  1798/  5000 (36%)]\tLearning rate: 0.0001\tLoss: 0.158996\n",
      "Train Epoch:  1 [  1998/  5000 (40%)]\tLearning rate: 0.0001\tLoss: 0.164698\n",
      "Train Epoch:  1 [  2198/  5000 (44%)]\tLearning rate: 0.0001\tLoss: 0.196179\n",
      "Train Epoch:  1 [  2398/  5000 (48%)]\tLearning rate: 0.0001\tLoss: 0.178447\n",
      "Train Epoch:  1 [  2598/  5000 (52%)]\tLearning rate: 0.0001\tLoss: 0.166602\n",
      "Train Epoch:  1 [  2798/  5000 (56%)]\tLearning rate: 0.0001\tLoss: 0.153097\n",
      "Train Epoch:  1 [  2998/  5000 (60%)]\tLearning rate: 0.0001\tLoss: 0.164618\n",
      "Train Epoch:  1 [  3198/  5000 (64%)]\tLearning rate: 0.0001\tLoss: 0.172066\n",
      "Train Epoch:  1 [  3398/  5000 (68%)]\tLearning rate: 0.0001\tLoss: 0.169186\n",
      "Train Epoch:  1 [  3598/  5000 (72%)]\tLearning rate: 0.0001\tLoss: 0.149772\n",
      "Train Epoch:  1 [  3798/  5000 (76%)]\tLearning rate: 0.0001\tLoss: 0.132473\n",
      "Train Epoch:  1 [  3998/  5000 (80%)]\tLearning rate: 0.0001\tLoss: 0.165831\n",
      "Train Epoch:  1 [  4198/  5000 (84%)]\tLearning rate: 0.0001\tLoss: 0.161212\n",
      "Train Epoch:  1 [  4398/  5000 (88%)]\tLearning rate: 0.0001\tLoss: 0.205700\n",
      "Train Epoch:  1 [  4598/  5000 (92%)]\tLearning rate: 0.0001\tLoss: 0.196392\n",
      "Train Epoch:  1 [  4798/  5000 (96%)]\tLearning rate: 0.0001\tLoss: 0.181034\n",
      "Train Epoch:  1 [  4998/  5000 (100%)]\tLearning rate: 0.0001\tLoss: 0.173220\n",
      "\n",
      "Validation set: Average loss: 0.164724\n",
      "\n",
      "Train Epoch:  2 [   198/  5000 (4%)]\tLearning rate: 0.0001\tLoss: 0.167501\n",
      "Train Epoch:  2 [   398/  5000 (8%)]\tLearning rate: 0.0001\tLoss: 0.169017\n",
      "Train Epoch:  2 [   598/  5000 (12%)]\tLearning rate: 0.0001\tLoss: 0.175921\n",
      "Train Epoch:  2 [   798/  5000 (16%)]\tLearning rate: 0.0001\tLoss: 0.188456\n",
      "Train Epoch:  2 [   998/  5000 (20%)]\tLearning rate: 0.0001\tLoss: 0.164844\n",
      "Train Epoch:  2 [  1198/  5000 (24%)]\tLearning rate: 0.0001\tLoss: 0.171871\n",
      "Train Epoch:  2 [  1398/  5000 (28%)]\tLearning rate: 0.0001\tLoss: 0.198184\n",
      "Train Epoch:  2 [  1598/  5000 (32%)]\tLearning rate: 0.0001\tLoss: 0.176021\n",
      "Train Epoch:  2 [  1798/  5000 (36%)]\tLearning rate: 0.0001\tLoss: 0.157777\n",
      "Train Epoch:  2 [  1998/  5000 (40%)]\tLearning rate: 0.0001\tLoss: 0.163197\n",
      "Train Epoch:  2 [  2198/  5000 (44%)]\tLearning rate: 0.0001\tLoss: 0.194550\n",
      "Train Epoch:  2 [  2398/  5000 (48%)]\tLearning rate: 0.0001\tLoss: 0.177023\n",
      "Train Epoch:  2 [  2598/  5000 (52%)]\tLearning rate: 0.0001\tLoss: 0.165352\n",
      "Train Epoch:  2 [  2798/  5000 (56%)]\tLearning rate: 0.0001\tLoss: 0.151600\n",
      "Train Epoch:  2 [  2998/  5000 (60%)]\tLearning rate: 0.0001\tLoss: 0.163361\n",
      "Train Epoch:  2 [  3198/  5000 (64%)]\tLearning rate: 0.0001\tLoss: 0.170728\n",
      "Train Epoch:  2 [  3398/  5000 (68%)]\tLearning rate: 0.0001\tLoss: 0.167717\n",
      "Train Epoch:  2 [  3598/  5000 (72%)]\tLearning rate: 0.0001\tLoss: 0.148206\n",
      "Train Epoch:  2 [  3798/  5000 (76%)]\tLearning rate: 0.0001\tLoss: 0.131048\n",
      "Train Epoch:  2 [  3998/  5000 (80%)]\tLearning rate: 0.0001\tLoss: 0.164699\n",
      "Train Epoch:  2 [  4198/  5000 (84%)]\tLearning rate: 0.0001\tLoss: 0.159857\n",
      "Train Epoch:  2 [  4398/  5000 (88%)]\tLearning rate: 0.0001\tLoss: 0.203616\n",
      "Train Epoch:  2 [  4598/  5000 (92%)]\tLearning rate: 0.0001\tLoss: 0.194134\n",
      "Train Epoch:  2 [  4798/  5000 (96%)]\tLearning rate: 0.0001\tLoss: 0.179281\n",
      "Train Epoch:  2 [  4998/  5000 (100%)]\tLearning rate: 0.0001\tLoss: 0.171879\n",
      "\n",
      "Validation set: Average loss: 0.163317\n",
      "\n",
      "Train Epoch:  3 [   198/  5000 (4%)]\tLearning rate: 0.0001\tLoss: 0.165987\n",
      "Train Epoch:  3 [   398/  5000 (8%)]\tLearning rate: 0.0001\tLoss: 0.167119\n",
      "Train Epoch:  3 [   598/  5000 (12%)]\tLearning rate: 0.0001\tLoss: 0.174222\n",
      "Train Epoch:  3 [   798/  5000 (16%)]\tLearning rate: 0.0001\tLoss: 0.186468\n",
      "Train Epoch:  3 [   998/  5000 (20%)]\tLearning rate: 0.0001\tLoss: 0.162974\n",
      "Train Epoch:  3 [  1198/  5000 (24%)]\tLearning rate: 0.0001\tLoss: 0.170043\n",
      "Train Epoch:  3 [  1398/  5000 (28%)]\tLearning rate: 0.0001\tLoss: 0.196284\n",
      "Train Epoch:  3 [  1598/  5000 (32%)]\tLearning rate: 0.0001\tLoss: 0.174656\n",
      "Train Epoch:  3 [  1798/  5000 (36%)]\tLearning rate: 0.0001\tLoss: 0.156221\n",
      "Train Epoch:  3 [  1998/  5000 (40%)]\tLearning rate: 0.0001\tLoss: 0.161468\n",
      "Train Epoch:  3 [  2198/  5000 (44%)]\tLearning rate: 0.0001\tLoss: 0.192666\n",
      "Train Epoch:  3 [  2398/  5000 (48%)]\tLearning rate: 0.0001\tLoss: 0.175194\n",
      "Train Epoch:  3 [  2598/  5000 (52%)]\tLearning rate: 0.0001\tLoss: 0.163778\n",
      "Train Epoch:  3 [  2798/  5000 (56%)]\tLearning rate: 0.0001\tLoss: 0.149876\n",
      "Train Epoch:  3 [  2998/  5000 (60%)]\tLearning rate: 0.0001\tLoss: 0.161764\n",
      "Train Epoch:  3 [  3198/  5000 (64%)]\tLearning rate: 0.0001\tLoss: 0.168924\n",
      "Train Epoch:  3 [  3398/  5000 (68%)]\tLearning rate: 0.0001\tLoss: 0.165924\n",
      "Train Epoch:  3 [  3598/  5000 (72%)]\tLearning rate: 0.0001\tLoss: 0.146436\n",
      "Train Epoch:  3 [  3798/  5000 (76%)]\tLearning rate: 0.0001\tLoss: 0.129457\n",
      "Train Epoch:  3 [  3998/  5000 (80%)]\tLearning rate: 0.0001\tLoss: 0.163167\n",
      "Train Epoch:  3 [  4198/  5000 (84%)]\tLearning rate: 0.0001\tLoss: 0.158234\n",
      "Train Epoch:  3 [  4398/  5000 (88%)]\tLearning rate: 0.0001\tLoss: 0.201178\n",
      "Train Epoch:  3 [  4598/  5000 (92%)]\tLearning rate: 0.0001\tLoss: 0.191423\n",
      "Train Epoch:  3 [  4798/  5000 (96%)]\tLearning rate: 0.0001\tLoss: 0.177126\n",
      "Train Epoch:  3 [  4998/  5000 (100%)]\tLearning rate: 0.0001\tLoss: 0.170113\n",
      "\n",
      "Validation set: Average loss: 0.161330\n",
      "\n",
      "Train Epoch:  4 [   198/  5000 (4%)]\tLearning rate: 0.0001\tLoss: 0.164092\n",
      "Train Epoch:  4 [   398/  5000 (8%)]\tLearning rate: 0.0001\tLoss: 0.165002\n",
      "Train Epoch:  4 [   598/  5000 (12%)]\tLearning rate: 0.0001\tLoss: 0.172067\n",
      "Train Epoch:  4 [   798/  5000 (16%)]\tLearning rate: 0.0001\tLoss: 0.184118\n",
      "Train Epoch:  4 [   998/  5000 (20%)]\tLearning rate: 0.0001\tLoss: 0.160781\n",
      "Train Epoch:  4 [  1198/  5000 (24%)]\tLearning rate: 0.0001\tLoss: 0.168046\n",
      "Train Epoch:  4 [  1398/  5000 (28%)]\tLearning rate: 0.0001\tLoss: 0.193888\n",
      "Train Epoch:  4 [  1598/  5000 (32%)]\tLearning rate: 0.0001\tLoss: 0.172793\n",
      "Train Epoch:  4 [  1798/  5000 (36%)]\tLearning rate: 0.0001\tLoss: 0.154225\n",
      "Train Epoch:  4 [  1998/  5000 (40%)]\tLearning rate: 0.0001\tLoss: 0.159301\n",
      "Train Epoch:  4 [  2198/  5000 (44%)]\tLearning rate: 0.0001\tLoss: 0.190430\n",
      "Train Epoch:  4 [  2398/  5000 (48%)]\tLearning rate: 0.0001\tLoss: 0.172772\n",
      "Train Epoch:  4 [  2598/  5000 (52%)]\tLearning rate: 0.0001\tLoss: 0.161934\n",
      "Train Epoch:  4 [  2798/  5000 (56%)]\tLearning rate: 0.0001\tLoss: 0.148089\n",
      "Train Epoch:  4 [  2998/  5000 (60%)]\tLearning rate: 0.0001\tLoss: 0.159788\n",
      "Train Epoch:  4 [  3198/  5000 (64%)]\tLearning rate: 0.0001\tLoss: 0.166574\n",
      "Train Epoch:  4 [  3398/  5000 (68%)]\tLearning rate: 0.0001\tLoss: 0.163705\n",
      "Train Epoch:  4 [  3598/  5000 (72%)]\tLearning rate: 0.0001\tLoss: 0.144370\n",
      "Train Epoch:  4 [  3798/  5000 (76%)]\tLearning rate: 0.0001\tLoss: 0.127451\n",
      "Train Epoch:  4 [  3998/  5000 (80%)]\tLearning rate: 0.0001\tLoss: 0.161034\n",
      "Train Epoch:  4 [  4198/  5000 (84%)]\tLearning rate: 0.0001\tLoss: 0.156173\n",
      "Train Epoch:  4 [  4398/  5000 (88%)]\tLearning rate: 0.0001\tLoss: 0.198402\n",
      "Train Epoch:  4 [  4598/  5000 (92%)]\tLearning rate: 0.0001\tLoss: 0.188048\n",
      "Train Epoch:  4 [  4798/  5000 (96%)]\tLearning rate: 0.0001\tLoss: 0.174397\n",
      "Train Epoch:  4 [  4998/  5000 (100%)]\tLearning rate: 0.0001\tLoss: 0.167783\n",
      "\n",
      "Validation set: Average loss: 0.158898\n",
      "\n",
      "Train Epoch:  5 [   198/  5000 (4%)]\tLearning rate: 0.0001\tLoss: 0.161882\n",
      "Train Epoch:  5 [   398/  5000 (8%)]\tLearning rate: 0.0001\tLoss: 0.162392\n",
      "Train Epoch:  5 [   598/  5000 (12%)]\tLearning rate: 0.0001\tLoss: 0.169512\n",
      "Train Epoch:  5 [   798/  5000 (16%)]\tLearning rate: 0.0001\tLoss: 0.181267\n",
      "Train Epoch:  5 [   998/  5000 (20%)]\tLearning rate: 0.0001\tLoss: 0.158045\n",
      "Train Epoch:  5 [  1198/  5000 (24%)]\tLearning rate: 0.0001\tLoss: 0.165477\n",
      "Train Epoch:  5 [  1398/  5000 (28%)]\tLearning rate: 0.0001\tLoss: 0.190766\n",
      "Train Epoch:  5 [  1598/  5000 (32%)]\tLearning rate: 0.0001\tLoss: 0.170348\n",
      "Train Epoch:  5 [  1798/  5000 (36%)]\tLearning rate: 0.0001\tLoss: 0.151690\n",
      "Train Epoch:  5 [  1998/  5000 (40%)]\tLearning rate: 0.0001\tLoss: 0.156493\n",
      "Train Epoch:  5 [  2198/  5000 (44%)]\tLearning rate: 0.0001\tLoss: 0.187512\n",
      "Train Epoch:  5 [  2398/  5000 (48%)]\tLearning rate: 0.0001\tLoss: 0.169527\n",
      "Train Epoch:  5 [  2598/  5000 (52%)]\tLearning rate: 0.0001\tLoss: 0.159418\n",
      "Train Epoch:  5 [  2798/  5000 (56%)]\tLearning rate: 0.0001\tLoss: 0.145941\n",
      "Train Epoch:  5 [  2998/  5000 (60%)]\tLearning rate: 0.0001\tLoss: 0.157317\n",
      "Train Epoch:  5 [  3198/  5000 (64%)]\tLearning rate: 0.0001\tLoss: 0.163443\n",
      "Train Epoch:  5 [  3398/  5000 (68%)]\tLearning rate: 0.0001\tLoss: 0.160914\n",
      "Train Epoch:  5 [  3598/  5000 (72%)]\tLearning rate: 0.0001\tLoss: 0.141716\n",
      "Train Epoch:  5 [  3798/  5000 (76%)]\tLearning rate: 0.0001\tLoss: 0.125066\n",
      "Train Epoch:  5 [  3998/  5000 (80%)]\tLearning rate: 0.0001\tLoss: 0.157950\n",
      "Train Epoch:  5 [  4198/  5000 (84%)]\tLearning rate: 0.0001\tLoss: 0.153400\n",
      "Train Epoch:  5 [  4398/  5000 (88%)]\tLearning rate: 0.0001\tLoss: 0.195035\n",
      "Train Epoch:  5 [  4598/  5000 (92%)]\tLearning rate: 0.0001\tLoss: 0.183813\n",
      "Train Epoch:  5 [  4798/  5000 (96%)]\tLearning rate: 0.0001\tLoss: 0.170953\n",
      "Train Epoch:  5 [  4998/  5000 (100%)]\tLearning rate: 0.0001\tLoss: 0.164371\n",
      "\n",
      "Validation set: Average loss: 0.155167\n",
      "\n",
      "Train Epoch:  6 [   198/  5000 (4%)]\tLearning rate: 0.0001\tLoss: 0.158825\n",
      "Train Epoch:  6 [   398/  5000 (8%)]\tLearning rate: 0.0001\tLoss: 0.158790\n",
      "Train Epoch:  6 [   598/  5000 (12%)]\tLearning rate: 0.0001\tLoss: 0.165914\n",
      "Train Epoch:  6 [   798/  5000 (16%)]\tLearning rate: 0.0001\tLoss: 0.177451\n",
      "Train Epoch:  6 [   998/  5000 (20%)]\tLearning rate: 0.0001\tLoss: 0.154227\n",
      "Train Epoch:  6 [  1198/  5000 (24%)]\tLearning rate: 0.0001\tLoss: 0.161978\n",
      "Train Epoch:  6 [  1398/  5000 (28%)]\tLearning rate: 0.0001\tLoss: 0.185984\n",
      "Train Epoch:  6 [  1598/  5000 (32%)]\tLearning rate: 0.0001\tLoss: 0.166588\n",
      "Train Epoch:  6 [  1798/  5000 (36%)]\tLearning rate: 0.0001\tLoss: 0.147905\n",
      "Train Epoch:  6 [  1998/  5000 (40%)]\tLearning rate: 0.0001\tLoss: 0.152102\n",
      "Train Epoch:  6 [  2198/  5000 (44%)]\tLearning rate: 0.0001\tLoss: 0.183073\n",
      "Train Epoch:  6 [  2398/  5000 (48%)]\tLearning rate: 0.0001\tLoss: 0.164235\n",
      "Train Epoch:  6 [  2598/  5000 (52%)]\tLearning rate: 0.0001\tLoss: 0.155338\n",
      "Train Epoch:  6 [  2798/  5000 (56%)]\tLearning rate: 0.0001\tLoss: 0.142844\n",
      "Train Epoch:  6 [  2998/  5000 (60%)]\tLearning rate: 0.0001\tLoss: 0.153081\n",
      "Train Epoch:  6 [  3198/  5000 (64%)]\tLearning rate: 0.0001\tLoss: 0.157718\n",
      "Train Epoch:  6 [  3398/  5000 (68%)]\tLearning rate: 0.0001\tLoss: 0.156197\n",
      "Train Epoch:  6 [  3598/  5000 (72%)]\tLearning rate: 0.0001\tLoss: 0.137154\n",
      "Train Epoch:  6 [  3798/  5000 (76%)]\tLearning rate: 0.0001\tLoss: 0.121214\n",
      "Train Epoch:  6 [  3998/  5000 (80%)]\tLearning rate: 0.0001\tLoss: 0.152010\n",
      "Train Epoch:  6 [  4198/  5000 (84%)]\tLearning rate: 0.0001\tLoss: 0.148141\n",
      "Train Epoch:  6 [  4398/  5000 (88%)]\tLearning rate: 0.0001\tLoss: 0.189265\n",
      "Train Epoch:  6 [  4598/  5000 (92%)]\tLearning rate: 0.0001\tLoss: 0.176320\n",
      "Train Epoch:  6 [  4798/  5000 (96%)]\tLearning rate: 0.0001\tLoss: 0.164670\n",
      "Train Epoch:  6 [  4998/  5000 (100%)]\tLearning rate: 0.0001\tLoss: 0.156984\n",
      "\n",
      "Validation set: Average loss: 0.147320\n",
      "\n",
      "Train Epoch:  7 [   198/  5000 (4%)]\tLearning rate: 0.0001\tLoss: 0.152530\n",
      "Train Epoch:  7 [   398/  5000 (8%)]\tLearning rate: 0.0001\tLoss: 0.151038\n",
      "Train Epoch:  7 [   598/  5000 (12%)]\tLearning rate: 0.0001\tLoss: 0.157548\n",
      "Train Epoch:  7 [   798/  5000 (16%)]\tLearning rate: 0.0001\tLoss: 0.168742\n",
      "Train Epoch:  7 [   998/  5000 (20%)]\tLearning rate: 0.0001\tLoss: 0.146149\n",
      "Train Epoch:  7 [  1198/  5000 (24%)]\tLearning rate: 0.0001\tLoss: 0.152656\n",
      "Train Epoch:  7 [  1398/  5000 (28%)]\tLearning rate: 0.0001\tLoss: 0.172406\n",
      "Train Epoch:  7 [  1598/  5000 (32%)]\tLearning rate: 0.0001\tLoss: 0.154625\n",
      "Train Epoch:  7 [  1798/  5000 (36%)]\tLearning rate: 0.0001\tLoss: 0.135463\n",
      "Train Epoch:  7 [  1998/  5000 (40%)]\tLearning rate: 0.0001\tLoss: 0.137300\n",
      "Train Epoch:  7 [  2198/  5000 (44%)]\tLearning rate: 0.0001\tLoss: 0.165062\n",
      "Train Epoch:  7 [  2398/  5000 (48%)]\tLearning rate: 0.0001\tLoss: 0.142422\n",
      "Train Epoch:  7 [  2598/  5000 (52%)]\tLearning rate: 0.0001\tLoss: 0.136587\n",
      "Train Epoch:  7 [  2798/  5000 (56%)]\tLearning rate: 0.0001\tLoss: 0.124291\n",
      "Train Epoch:  7 [  2998/  5000 (60%)]\tLearning rate: 0.0001\tLoss: 0.128501\n",
      "Train Epoch:  7 [  3198/  5000 (64%)]\tLearning rate: 0.0001\tLoss: 0.121103\n",
      "Train Epoch:  7 [  3398/  5000 (68%)]\tLearning rate: 0.0001\tLoss: 0.114823\n",
      "Train Epoch:  7 [  3598/  5000 (72%)]\tLearning rate: 0.0001\tLoss: 0.090117\n",
      "Train Epoch:  7 [  3798/  5000 (76%)]\tLearning rate: 0.0001\tLoss: 0.072571\n",
      "Train Epoch:  7 [  3998/  5000 (80%)]\tLearning rate: 0.0001\tLoss: 0.074572\n",
      "Train Epoch:  7 [  4198/  5000 (84%)]\tLearning rate: 0.0001\tLoss: 0.053903\n",
      "Train Epoch:  7 [  4398/  5000 (88%)]\tLearning rate: 0.0001\tLoss: 0.039275\n",
      "Train Epoch:  7 [  4598/  5000 (92%)]\tLearning rate: 0.0001\tLoss: 0.039240\n",
      "Train Epoch:  7 [  4798/  5000 (96%)]\tLearning rate: 0.0001\tLoss: 0.037835\n",
      "Train Epoch:  7 [  4998/  5000 (100%)]\tLearning rate: 0.0001\tLoss: 0.027641\n",
      "\n",
      "Validation set: Average loss: 0.045517\n",
      "\n",
      "Train Epoch:  8 [   198/  5000 (4%)]\tLearning rate: 0.0001\tLoss: 0.022078\n",
      "Train Epoch:  8 [   398/  5000 (8%)]\tLearning rate: 0.0001\tLoss: 0.022952\n",
      "Train Epoch:  8 [   598/  5000 (12%)]\tLearning rate: 0.0001\tLoss: 0.017839\n",
      "Train Epoch:  8 [   798/  5000 (16%)]\tLearning rate: 0.0001\tLoss: 0.019641\n",
      "Train Epoch:  8 [   998/  5000 (20%)]\tLearning rate: 0.0001\tLoss: 0.019839\n",
      "Train Epoch:  8 [  1198/  5000 (24%)]\tLearning rate: 0.0001\tLoss: 0.013894\n",
      "Train Epoch:  8 [  1398/  5000 (28%)]\tLearning rate: 0.0001\tLoss: 0.019057\n",
      "Train Epoch:  8 [  1598/  5000 (32%)]\tLearning rate: 0.0001\tLoss: 0.016972\n",
      "Train Epoch:  8 [  1798/  5000 (36%)]\tLearning rate: 0.0001\tLoss: 0.010321\n",
      "Train Epoch:  8 [  1998/  5000 (40%)]\tLearning rate: 0.0001\tLoss: 0.014780\n",
      "Train Epoch:  8 [  2198/  5000 (44%)]\tLearning rate: 0.0001\tLoss: 0.017889\n",
      "Train Epoch:  8 [  2398/  5000 (48%)]\tLearning rate: 0.0001\tLoss: 0.013286\n",
      "Train Epoch:  8 [  2598/  5000 (52%)]\tLearning rate: 0.0001\tLoss: 0.017607\n",
      "Train Epoch:  8 [  2798/  5000 (56%)]\tLearning rate: 0.0001\tLoss: 0.013189\n",
      "Train Epoch:  8 [  2998/  5000 (60%)]\tLearning rate: 0.0001\tLoss: 0.020162\n",
      "Train Epoch:  8 [  3198/  5000 (64%)]\tLearning rate: 0.0001\tLoss: 0.012396\n",
      "Train Epoch:  8 [  3398/  5000 (68%)]\tLearning rate: 0.0001\tLoss: 0.012009\n",
      "Train Epoch:  8 [  3598/  5000 (72%)]\tLearning rate: 0.0001\tLoss: 0.011355\n",
      "Train Epoch:  8 [  3798/  5000 (76%)]\tLearning rate: 0.0001\tLoss: 0.013316\n",
      "Train Epoch:  8 [  3998/  5000 (80%)]\tLearning rate: 0.0001\tLoss: 0.013404\n",
      "Train Epoch:  8 [  4198/  5000 (84%)]\tLearning rate: 0.0001\tLoss: 0.009042\n",
      "Train Epoch:  8 [  4398/  5000 (88%)]\tLearning rate: 0.0001\tLoss: 0.006942\n",
      "Train Epoch:  8 [  4598/  5000 (92%)]\tLearning rate: 0.0001\tLoss: 0.011685\n",
      "Train Epoch:  8 [  4798/  5000 (96%)]\tLearning rate: 0.0001\tLoss: 0.014696\n",
      "Train Epoch:  8 [  4998/  5000 (100%)]\tLearning rate: 0.0001\tLoss: 0.012237\n",
      "\n",
      "Validation set: Average loss: 0.009107\n",
      "\n",
      "Train Epoch:  9 [   198/  5000 (4%)]\tLearning rate: 0.0001\tLoss: 0.009000\n",
      "Train Epoch:  9 [   398/  5000 (8%)]\tLearning rate: 0.0001\tLoss: 0.010158\n",
      "Train Epoch:  9 [   598/  5000 (12%)]\tLearning rate: 0.0001\tLoss: 0.008829\n",
      "Train Epoch:  9 [   798/  5000 (16%)]\tLearning rate: 0.0001\tLoss: 0.008031\n",
      "Train Epoch:  9 [   998/  5000 (20%)]\tLearning rate: 0.0001\tLoss: 0.009220\n",
      "Train Epoch:  9 [  1198/  5000 (24%)]\tLearning rate: 0.0001\tLoss: 0.005579\n",
      "Train Epoch:  9 [  1398/  5000 (28%)]\tLearning rate: 0.0001\tLoss: 0.010493\n",
      "Train Epoch:  9 [  1598/  5000 (32%)]\tLearning rate: 0.0001\tLoss: 0.007681\n",
      "Train Epoch:  9 [  1798/  5000 (36%)]\tLearning rate: 0.0001\tLoss: 0.005153\n",
      "Train Epoch:  9 [  1998/  5000 (40%)]\tLearning rate: 0.0001\tLoss: 0.005300\n",
      "Train Epoch:  9 [  2198/  5000 (44%)]\tLearning rate: 0.0001\tLoss: 0.008271\n",
      "Train Epoch:  9 [  2398/  5000 (48%)]\tLearning rate: 0.0001\tLoss: 0.007175\n",
      "Train Epoch:  9 [  2598/  5000 (52%)]\tLearning rate: 0.0001\tLoss: 0.009706\n",
      "Train Epoch:  9 [  2798/  5000 (56%)]\tLearning rate: 0.0001\tLoss: 0.006305\n",
      "Train Epoch:  9 [  2998/  5000 (60%)]\tLearning rate: 0.0001\tLoss: 0.009968\n",
      "Train Epoch:  9 [  3198/  5000 (64%)]\tLearning rate: 0.0001\tLoss: 0.006065\n",
      "Train Epoch:  9 [  3398/  5000 (68%)]\tLearning rate: 0.0001\tLoss: 0.006492\n",
      "Train Epoch:  9 [  3598/  5000 (72%)]\tLearning rate: 0.0001\tLoss: 0.004983\n",
      "Train Epoch:  9 [  3798/  5000 (76%)]\tLearning rate: 0.0001\tLoss: 0.007399\n",
      "Train Epoch:  9 [  3998/  5000 (80%)]\tLearning rate: 0.0001\tLoss: 0.007200\n",
      "Train Epoch:  9 [  4198/  5000 (84%)]\tLearning rate: 0.0001\tLoss: 0.004696\n",
      "Train Epoch:  9 [  4398/  5000 (88%)]\tLearning rate: 0.0001\tLoss: 0.003768\n",
      "Train Epoch:  9 [  4598/  5000 (92%)]\tLearning rate: 0.0001\tLoss: 0.005910\n",
      "Train Epoch:  9 [  4798/  5000 (96%)]\tLearning rate: 0.0001\tLoss: 0.006496\n",
      "Train Epoch:  9 [  4998/  5000 (100%)]\tLearning rate: 0.0001\tLoss: 0.006746\n",
      "\n",
      "Validation set: Average loss: 0.004214\n",
      "\n",
      "Train Epoch: 10 [   198/  5000 (4%)]\tLearning rate: 0.0001\tLoss: 0.005521\n",
      "Train Epoch: 10 [   398/  5000 (8%)]\tLearning rate: 0.0001\tLoss: 0.005195\n",
      "Train Epoch: 10 [   598/  5000 (12%)]\tLearning rate: 0.0001\tLoss: 0.004098\n",
      "Train Epoch: 10 [   798/  5000 (16%)]\tLearning rate: 0.0001\tLoss: 0.005109\n",
      "Train Epoch: 10 [   998/  5000 (20%)]\tLearning rate: 0.0001\tLoss: 0.004944\n",
      "Train Epoch: 10 [  1198/  5000 (24%)]\tLearning rate: 0.0001\tLoss: 0.003264\n",
      "Train Epoch: 10 [  1398/  5000 (28%)]\tLearning rate: 0.0001\tLoss: 0.005939\n",
      "Train Epoch: 10 [  1598/  5000 (32%)]\tLearning rate: 0.0001\tLoss: 0.004116\n",
      "Train Epoch: 10 [  1798/  5000 (36%)]\tLearning rate: 0.0001\tLoss: 0.002986\n",
      "Train Epoch: 10 [  1998/  5000 (40%)]\tLearning rate: 0.0001\tLoss: 0.003339\n",
      "Train Epoch: 10 [  2198/  5000 (44%)]\tLearning rate: 0.0001\tLoss: 0.004626\n",
      "Train Epoch: 10 [  2398/  5000 (48%)]\tLearning rate: 0.0001\tLoss: 0.004288\n",
      "Train Epoch: 10 [  2598/  5000 (52%)]\tLearning rate: 0.0001\tLoss: 0.005084\n",
      "Train Epoch: 10 [  2798/  5000 (56%)]\tLearning rate: 0.0001\tLoss: 0.003772\n",
      "Train Epoch: 10 [  2998/  5000 (60%)]\tLearning rate: 0.0001\tLoss: 0.004806\n",
      "Train Epoch: 10 [  3198/  5000 (64%)]\tLearning rate: 0.0001\tLoss: 0.003718\n",
      "Train Epoch: 10 [  3398/  5000 (68%)]\tLearning rate: 0.0001\tLoss: 0.003933\n",
      "Train Epoch: 10 [  3598/  5000 (72%)]\tLearning rate: 0.0001\tLoss: 0.002934\n",
      "Train Epoch: 10 [  3798/  5000 (76%)]\tLearning rate: 0.0001\tLoss: 0.004506\n",
      "Train Epoch: 10 [  3998/  5000 (80%)]\tLearning rate: 0.0001\tLoss: 0.004410\n",
      "Train Epoch: 10 [  4198/  5000 (84%)]\tLearning rate: 0.0001\tLoss: 0.002976\n",
      "Train Epoch: 10 [  4398/  5000 (88%)]\tLearning rate: 0.0001\tLoss: 0.002224\n",
      "Train Epoch: 10 [  4598/  5000 (92%)]\tLearning rate: 0.0001\tLoss: 0.003663\n",
      "Train Epoch: 10 [  4798/  5000 (96%)]\tLearning rate: 0.0001\tLoss: 0.003596\n",
      "Train Epoch: 10 [  4998/  5000 (100%)]\tLearning rate: 0.0001\tLoss: 0.003835\n",
      "\n",
      "Validation set: Average loss: 0.003237\n",
      "\n",
      "Train Epoch: 11 [   198/  5000 (4%)]\tLearning rate: 0.0001\tLoss: 0.003402\n",
      "Train Epoch: 11 [   398/  5000 (8%)]\tLearning rate: 0.0001\tLoss: 0.003800\n",
      "Train Epoch: 11 [   598/  5000 (12%)]\tLearning rate: 0.0001\tLoss: 0.002597\n",
      "Train Epoch: 11 [   798/  5000 (16%)]\tLearning rate: 0.0001\tLoss: 0.003317\n",
      "Train Epoch: 11 [   998/  5000 (20%)]\tLearning rate: 0.0001\tLoss: 0.002875\n",
      "Train Epoch: 11 [  1198/  5000 (24%)]\tLearning rate: 0.0001\tLoss: 0.002287\n",
      "Train Epoch: 11 [  1398/  5000 (28%)]\tLearning rate: 0.0001\tLoss: 0.003653\n",
      "Train Epoch: 11 [  1598/  5000 (32%)]\tLearning rate: 0.0001\tLoss: 0.002565\n",
      "Train Epoch: 11 [  1798/  5000 (36%)]\tLearning rate: 0.0001\tLoss: 0.002369\n",
      "Train Epoch: 11 [  1998/  5000 (40%)]\tLearning rate: 0.0001\tLoss: 0.002614\n",
      "Train Epoch: 11 [  2198/  5000 (44%)]\tLearning rate: 0.0001\tLoss: 0.002871\n",
      "Train Epoch: 11 [  2398/  5000 (48%)]\tLearning rate: 0.0001\tLoss: 0.002676\n",
      "Train Epoch: 11 [  2598/  5000 (52%)]\tLearning rate: 0.0001\tLoss: 0.003509\n",
      "Train Epoch: 11 [  2798/  5000 (56%)]\tLearning rate: 0.0001\tLoss: 0.002425\n",
      "Train Epoch: 11 [  2998/  5000 (60%)]\tLearning rate: 0.0001\tLoss: 0.002762\n",
      "Train Epoch: 11 [  3198/  5000 (64%)]\tLearning rate: 0.0001\tLoss: 0.002381\n",
      "Train Epoch: 11 [  3398/  5000 (68%)]\tLearning rate: 0.0001\tLoss: 0.002626\n",
      "Train Epoch: 11 [  3598/  5000 (72%)]\tLearning rate: 0.0001\tLoss: 0.001916\n",
      "Train Epoch: 11 [  3798/  5000 (76%)]\tLearning rate: 0.0001\tLoss: 0.003060\n",
      "Train Epoch: 11 [  3998/  5000 (80%)]\tLearning rate: 0.0001\tLoss: 0.003499\n",
      "Train Epoch: 11 [  4198/  5000 (84%)]\tLearning rate: 0.0001\tLoss: 0.002193\n",
      "Train Epoch: 11 [  4398/  5000 (88%)]\tLearning rate: 0.0001\tLoss: 0.001615\n",
      "Train Epoch: 11 [  4598/  5000 (92%)]\tLearning rate: 0.0001\tLoss: 0.002661\n",
      "Train Epoch: 11 [  4798/  5000 (96%)]\tLearning rate: 0.0001\tLoss: 0.002192\n",
      "Train Epoch: 11 [  4998/  5000 (100%)]\tLearning rate: 0.0001\tLoss: 0.002385\n",
      "\n",
      "Validation set: Average loss: 0.002020\n",
      "\n",
      "Train Epoch: 12 [   198/  5000 (4%)]\tLearning rate: 0.0001\tLoss: 0.002527\n",
      "Train Epoch: 12 [   398/  5000 (8%)]\tLearning rate: 0.0001\tLoss: 0.003215\n",
      "Train Epoch: 12 [   598/  5000 (12%)]\tLearning rate: 0.0001\tLoss: 0.001983\n",
      "Train Epoch: 12 [   798/  5000 (16%)]\tLearning rate: 0.0001\tLoss: 0.002506\n",
      "Train Epoch: 12 [   998/  5000 (20%)]\tLearning rate: 0.0001\tLoss: 0.001906\n",
      "Train Epoch: 12 [  1198/  5000 (24%)]\tLearning rate: 0.0001\tLoss: 0.001547\n",
      "Train Epoch: 12 [  1398/  5000 (28%)]\tLearning rate: 0.0001\tLoss: 0.002097\n",
      "Train Epoch: 12 [  1598/  5000 (32%)]\tLearning rate: 0.0001\tLoss: 0.001788\n",
      "Train Epoch: 12 [  1798/  5000 (36%)]\tLearning rate: 0.0001\tLoss: 0.001781\n",
      "Train Epoch: 12 [  1998/  5000 (40%)]\tLearning rate: 0.0001\tLoss: 0.002142\n",
      "Train Epoch: 12 [  2198/  5000 (44%)]\tLearning rate: 0.0001\tLoss: 0.002006\n",
      "Train Epoch: 12 [  2398/  5000 (48%)]\tLearning rate: 0.0001\tLoss: 0.001800\n",
      "Train Epoch: 12 [  2598/  5000 (52%)]\tLearning rate: 0.0001\tLoss: 0.002554\n",
      "Train Epoch: 12 [  2798/  5000 (56%)]\tLearning rate: 0.0001\tLoss: 0.001614\n",
      "Train Epoch: 12 [  2998/  5000 (60%)]\tLearning rate: 0.0001\tLoss: 0.001921\n",
      "Train Epoch: 12 [  3198/  5000 (64%)]\tLearning rate: 0.0001\tLoss: 0.001668\n",
      "Train Epoch: 12 [  3398/  5000 (68%)]\tLearning rate: 0.0001\tLoss: 0.001710\n",
      "Train Epoch: 12 [  3598/  5000 (72%)]\tLearning rate: 0.0001\tLoss: 0.001642\n",
      "Train Epoch: 12 [  3798/  5000 (76%)]\tLearning rate: 0.0001\tLoss: 0.002763\n",
      "Train Epoch: 12 [  3998/  5000 (80%)]\tLearning rate: 0.0001\tLoss: 0.002718\n",
      "Train Epoch: 12 [  4198/  5000 (84%)]\tLearning rate: 0.0001\tLoss: 0.001662\n",
      "Train Epoch: 12 [  4398/  5000 (88%)]\tLearning rate: 0.0001\tLoss: 0.001259\n",
      "Train Epoch: 12 [  4598/  5000 (92%)]\tLearning rate: 0.0001\tLoss: 0.002026\n",
      "Train Epoch: 12 [  4798/  5000 (96%)]\tLearning rate: 0.0001\tLoss: 0.001515\n",
      "Train Epoch: 12 [  4998/  5000 (100%)]\tLearning rate: 0.0001\tLoss: 0.001583\n",
      "\n",
      "Validation set: Average loss: 0.001419\n",
      "\n",
      "Train Epoch: 13 [   198/  5000 (4%)]\tLearning rate: 0.0001\tLoss: 0.002082\n",
      "Train Epoch: 13 [   398/  5000 (8%)]\tLearning rate: 0.0001\tLoss: 0.002726\n",
      "Train Epoch: 13 [   598/  5000 (12%)]\tLearning rate: 0.0001\tLoss: 0.001456\n",
      "Train Epoch: 13 [   798/  5000 (16%)]\tLearning rate: 0.0001\tLoss: 0.002012\n",
      "Train Epoch: 13 [   998/  5000 (20%)]\tLearning rate: 0.0001\tLoss: 0.001386\n",
      "Train Epoch: 13 [  1198/  5000 (24%)]\tLearning rate: 0.0001\tLoss: 0.001113\n",
      "Train Epoch: 13 [  1398/  5000 (28%)]\tLearning rate: 0.0001\tLoss: 0.001369\n",
      "Train Epoch: 13 [  1598/  5000 (32%)]\tLearning rate: 0.0001\tLoss: 0.001342\n",
      "Train Epoch: 13 [  1798/  5000 (36%)]\tLearning rate: 0.0001\tLoss: 0.001326\n",
      "Train Epoch: 13 [  1998/  5000 (40%)]\tLearning rate: 0.0001\tLoss: 0.001743\n",
      "Train Epoch: 13 [  2198/  5000 (44%)]\tLearning rate: 0.0001\tLoss: 0.001641\n",
      "Train Epoch: 13 [  2398/  5000 (48%)]\tLearning rate: 0.0001\tLoss: 0.001273\n",
      "Train Epoch: 13 [  2598/  5000 (52%)]\tLearning rate: 0.0001\tLoss: 0.001995\n",
      "Train Epoch: 13 [  2798/  5000 (56%)]\tLearning rate: 0.0001\tLoss: 0.001139\n",
      "Train Epoch: 13 [  2998/  5000 (60%)]\tLearning rate: 0.0001\tLoss: 0.001488\n",
      "Train Epoch: 13 [  3198/  5000 (64%)]\tLearning rate: 0.0001\tLoss: 0.001216\n",
      "Train Epoch: 13 [  3398/  5000 (68%)]\tLearning rate: 0.0001\tLoss: 0.001354\n",
      "Train Epoch: 13 [  3598/  5000 (72%)]\tLearning rate: 0.0001\tLoss: 0.001399\n",
      "Train Epoch: 13 [  3798/  5000 (76%)]\tLearning rate: 0.0001\tLoss: 0.002333\n",
      "Train Epoch: 13 [  3998/  5000 (80%)]\tLearning rate: 0.0001\tLoss: 0.002086\n",
      "Train Epoch: 13 [  4198/  5000 (84%)]\tLearning rate: 0.0001\tLoss: 0.001328\n",
      "Train Epoch: 13 [  4398/  5000 (88%)]\tLearning rate: 0.0001\tLoss: 0.001077\n",
      "Train Epoch: 13 [  4598/  5000 (92%)]\tLearning rate: 0.0001\tLoss: 0.001523\n",
      "Train Epoch: 13 [  4798/  5000 (96%)]\tLearning rate: 0.0001\tLoss: 0.001118\n",
      "Train Epoch: 13 [  4998/  5000 (100%)]\tLearning rate: 0.0001\tLoss: 0.001068\n",
      "\n",
      "Validation set: Average loss: 0.001004\n",
      "\n",
      "Train Epoch: 14 [   198/  5000 (4%)]\tLearning rate: 0.0001\tLoss: 0.001934\n",
      "Train Epoch: 14 [   398/  5000 (8%)]\tLearning rate: 0.0001\tLoss: 0.002193\n",
      "Train Epoch: 14 [   598/  5000 (12%)]\tLearning rate: 0.0001\tLoss: 0.001149\n",
      "Train Epoch: 14 [   798/  5000 (16%)]\tLearning rate: 0.0001\tLoss: 0.001604\n",
      "Train Epoch: 14 [   998/  5000 (20%)]\tLearning rate: 0.0001\tLoss: 0.001047\n",
      "Train Epoch: 14 [  1198/  5000 (24%)]\tLearning rate: 0.0001\tLoss: 0.000815\n",
      "Train Epoch: 14 [  1398/  5000 (28%)]\tLearning rate: 0.0001\tLoss: 0.001023\n",
      "Train Epoch: 14 [  1598/  5000 (32%)]\tLearning rate: 0.0001\tLoss: 0.001054\n",
      "Train Epoch: 14 [  1798/  5000 (36%)]\tLearning rate: 0.0001\tLoss: 0.001024\n",
      "Train Epoch: 14 [  1998/  5000 (40%)]\tLearning rate: 0.0001\tLoss: 0.001509\n",
      "Train Epoch: 14 [  2198/  5000 (44%)]\tLearning rate: 0.0001\tLoss: 0.001369\n",
      "Train Epoch: 14 [  2398/  5000 (48%)]\tLearning rate: 0.0001\tLoss: 0.000942\n",
      "Train Epoch: 14 [  2598/  5000 (52%)]\tLearning rate: 0.0001\tLoss: 0.001620\n",
      "Train Epoch: 14 [  2798/  5000 (56%)]\tLearning rate: 0.0001\tLoss: 0.000927\n",
      "Train Epoch: 14 [  2998/  5000 (60%)]\tLearning rate: 0.0001\tLoss: 0.001184\n",
      "Train Epoch: 14 [  3198/  5000 (64%)]\tLearning rate: 0.0001\tLoss: 0.000985\n",
      "Train Epoch: 14 [  3398/  5000 (68%)]\tLearning rate: 0.0001\tLoss: 0.001108\n",
      "Train Epoch: 14 [  3598/  5000 (72%)]\tLearning rate: 0.0001\tLoss: 0.001243\n",
      "Train Epoch: 14 [  3798/  5000 (76%)]\tLearning rate: 0.0001\tLoss: 0.001764\n",
      "Train Epoch: 14 [  3998/  5000 (80%)]\tLearning rate: 0.0001\tLoss: 0.001666\n",
      "Train Epoch: 14 [  4198/  5000 (84%)]\tLearning rate: 0.0001\tLoss: 0.001006\n",
      "Train Epoch: 14 [  4398/  5000 (88%)]\tLearning rate: 0.0001\tLoss: 0.000875\n",
      "Train Epoch: 14 [  4598/  5000 (92%)]\tLearning rate: 0.0001\tLoss: 0.001313\n",
      "Train Epoch: 14 [  4798/  5000 (96%)]\tLearning rate: 0.0001\tLoss: 0.000882\n",
      "Train Epoch: 14 [  4998/  5000 (100%)]\tLearning rate: 0.0001\tLoss: 0.000817\n",
      "\n",
      "Validation set: Average loss: 0.000734\n",
      "\n",
      "Train Epoch: 15 [   198/  5000 (4%)]\tLearning rate: 0.0001\tLoss: 0.001807\n",
      "Train Epoch: 15 [   398/  5000 (8%)]\tLearning rate: 0.0001\tLoss: 0.001801\n",
      "Train Epoch: 15 [   598/  5000 (12%)]\tLearning rate: 0.0001\tLoss: 0.001005\n",
      "Train Epoch: 15 [   798/  5000 (16%)]\tLearning rate: 0.0001\tLoss: 0.001374\n",
      "Train Epoch: 15 [   998/  5000 (20%)]\tLearning rate: 0.0001\tLoss: 0.000856\n",
      "Train Epoch: 15 [  1198/  5000 (24%)]\tLearning rate: 0.0001\tLoss: 0.000662\n",
      "Train Epoch: 15 [  1398/  5000 (28%)]\tLearning rate: 0.0001\tLoss: 0.000715\n",
      "Train Epoch: 15 [  1598/  5000 (32%)]\tLearning rate: 0.0001\tLoss: 0.000908\n",
      "Train Epoch: 15 [  1798/  5000 (36%)]\tLearning rate: 0.0001\tLoss: 0.000849\n",
      "Train Epoch: 15 [  1998/  5000 (40%)]\tLearning rate: 0.0001\tLoss: 0.001324\n",
      "Train Epoch: 15 [  2198/  5000 (44%)]\tLearning rate: 0.0001\tLoss: 0.001068\n",
      "Train Epoch: 15 [  2398/  5000 (48%)]\tLearning rate: 0.0001\tLoss: 0.000815\n",
      "Train Epoch: 15 [  2598/  5000 (52%)]\tLearning rate: 0.0001\tLoss: 0.001401\n",
      "Train Epoch: 15 [  2798/  5000 (56%)]\tLearning rate: 0.0001\tLoss: 0.000807\n",
      "Train Epoch: 15 [  2998/  5000 (60%)]\tLearning rate: 0.0001\tLoss: 0.000974\n",
      "Train Epoch: 15 [  3198/  5000 (64%)]\tLearning rate: 0.0001\tLoss: 0.000852\n",
      "Train Epoch: 15 [  3398/  5000 (68%)]\tLearning rate: 0.0001\tLoss: 0.000986\n",
      "Train Epoch: 15 [  3598/  5000 (72%)]\tLearning rate: 0.0001\tLoss: 0.001142\n",
      "Train Epoch: 15 [  3798/  5000 (76%)]\tLearning rate: 0.0001\tLoss: 0.001284\n",
      "Train Epoch: 15 [  3998/  5000 (80%)]\tLearning rate: 0.0001\tLoss: 0.001421\n",
      "Train Epoch: 15 [  4198/  5000 (84%)]\tLearning rate: 0.0001\tLoss: 0.000864\n",
      "Train Epoch: 15 [  4398/  5000 (88%)]\tLearning rate: 0.0001\tLoss: 0.000826\n",
      "Train Epoch: 15 [  4598/  5000 (92%)]\tLearning rate: 0.0001\tLoss: 0.001119\n",
      "Train Epoch: 15 [  4798/  5000 (96%)]\tLearning rate: 0.0001\tLoss: 0.000744\n",
      "Train Epoch: 15 [  4998/  5000 (100%)]\tLearning rate: 0.0001\tLoss: 0.000717\n",
      "\n",
      "Validation set: Average loss: 0.000602\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# train the model with two conv layers\n",
    "torch.manual_seed(args.seed)\n",
    "if torch.cuda.is_available():\n",
    "    if not args.cuda:\n",
    "        print(\"WARNING: You have a CUDA device, so you should probably run with --cuda\")\n",
    "\n",
    "input_channels = 2\n",
    "n_classes = 1\n",
    "batch_size = args.batch_size\n",
    "seq_length = args.seq_len\n",
    "epochs = args.epochs\n",
    "\n",
    "\n",
    "# Note: We use a very simple setting here (assuming all levels have the same # of channels.\n",
    "channel_sizes = [args.nhid]*args.levels\n",
    "kernel_size = args.ksize\n",
    "dropout = args.dropout\n",
    "model = TCN(input_channels, n_classes, channel_sizes, kernel_size=kernel_size, dropout=dropout)\n",
    "\n",
    "if args.cuda:\n",
    "    model.cuda()\n",
    "    X_train = X_train.cuda()\n",
    "    Y_train = Y_train.cuda()\n",
    "    X_test = X_test.cuda()\n",
    "    Y_test = Y_test.cuda()\n",
    "\n",
    "lr = args.lr\n",
    "optimizer = getattr(optim, args.optim)(model.parameters(), lr=lr)\n",
    "\n",
    "\n",
    "def train(epoch):\n",
    "    global lr\n",
    "    model.train()\n",
    "    batch_idx = 1\n",
    "    total_loss = 0\n",
    "    for i in range(0, X_train.size(0), batch_size):\n",
    "        if i + batch_size > X_train.size(0):\n",
    "            x, y = X_train[i:], Y_train[i:]\n",
    "        else:\n",
    "            x, y = X_train[i:(i+batch_size)], Y_train[i:(i+batch_size)]\n",
    "        optimizer.zero_grad()\n",
    "        output = model(x)\n",
    "        loss = F.mse_loss(output, y)\n",
    "        loss.backward()\n",
    "        if args.clip > 0:\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), args.clip)\n",
    "        optimizer.step()\n",
    "        batch_idx += 1\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        if batch_idx % args.log_interval == 0:\n",
    "            cur_loss = total_loss / args.log_interval\n",
    "            processed = min(i+batch_size, X_train.size(0))\n",
    "            print('Train Epoch: {:2d} [{:6d}/{:6d} ({:.0f}%)]\\tLearning rate: {:.4f}\\tLoss: {:.6f}'.format(\n",
    "                epoch, processed, X_train.size(0), 100.*processed/X_train.size(0), lr, cur_loss))\n",
    "            total_loss = 0\n",
    "\n",
    "\n",
    "def evaluate():\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        output = model(X_test)\n",
    "        test_loss = F.mse_loss(output, Y_test)\n",
    "        print('\\nValidation set: Average loss: {:.6f}\\n'.format(test_loss.item()))\n",
    "        return test_loss.item()\n",
    "\n",
    "\n",
    "for ep in range(1, epochs+1):\n",
    "    train(ep)\n",
    "    tloss = evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {
    "id": "IzBGs4L5HJpw"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Difference: 0.019098716229200362\n"
     ]
    }
   ],
   "source": [
    "# show the results. You will get full credit if the average differences is less than 0.02\n",
    "preds = model(X_test)\n",
    "\n",
    "total_diff = 0\n",
    "for i,pred in enumerate(preds):\n",
    "    total_diff += np.abs(pred.data.item() - Y_test[i].item())\n",
    "\n",
    "\n",
    "print('Average Difference:', total_diff/len(Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JXArmQQ-FtQO"
   },
   "source": [
    "**2(c)** Now build a TCN model with only one convolutional layer. Use the same parameter you set in 2(b) and train your network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {
    "id": "F_B0bVmKsWIK"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lawr_xd/anaconda3/envs/BMENE4470/lib/python3.8/site-packages/torch/nn/utils/weight_norm.py:30: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\n",
      "  warnings.warn(\"torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch:  1 [   198/  5000 (4%)]\tLearning rate: 0.0001\tLoss: 0.686307\n",
      "Train Epoch:  1 [   398/  5000 (8%)]\tLearning rate: 0.0001\tLoss: 0.170621\n",
      "Train Epoch:  1 [   598/  5000 (12%)]\tLearning rate: 0.0001\tLoss: 0.176809\n",
      "Train Epoch:  1 [   798/  5000 (16%)]\tLearning rate: 0.0001\tLoss: 0.187706\n",
      "Train Epoch:  1 [   998/  5000 (20%)]\tLearning rate: 0.0001\tLoss: 0.165829\n",
      "Train Epoch:  1 [  1198/  5000 (24%)]\tLearning rate: 0.0001\tLoss: 0.173168\n",
      "Train Epoch:  1 [  1398/  5000 (28%)]\tLearning rate: 0.0001\tLoss: 0.200057\n",
      "Train Epoch:  1 [  1598/  5000 (32%)]\tLearning rate: 0.0001\tLoss: 0.177867\n",
      "Train Epoch:  1 [  1798/  5000 (36%)]\tLearning rate: 0.0001\tLoss: 0.159763\n",
      "Train Epoch:  1 [  1998/  5000 (40%)]\tLearning rate: 0.0001\tLoss: 0.164051\n",
      "Train Epoch:  1 [  2198/  5000 (44%)]\tLearning rate: 0.0001\tLoss: 0.196170\n",
      "Train Epoch:  1 [  2398/  5000 (48%)]\tLearning rate: 0.0001\tLoss: 0.178390\n",
      "Train Epoch:  1 [  2598/  5000 (52%)]\tLearning rate: 0.0001\tLoss: 0.166467\n",
      "Train Epoch:  1 [  2798/  5000 (56%)]\tLearning rate: 0.0001\tLoss: 0.152877\n",
      "Train Epoch:  1 [  2998/  5000 (60%)]\tLearning rate: 0.0001\tLoss: 0.165906\n",
      "Train Epoch:  1 [  3198/  5000 (64%)]\tLearning rate: 0.0001\tLoss: 0.172022\n",
      "Train Epoch:  1 [  3398/  5000 (68%)]\tLearning rate: 0.0001\tLoss: 0.168387\n",
      "Train Epoch:  1 [  3598/  5000 (72%)]\tLearning rate: 0.0001\tLoss: 0.148944\n",
      "Train Epoch:  1 [  3798/  5000 (76%)]\tLearning rate: 0.0001\tLoss: 0.131629\n",
      "Train Epoch:  1 [  3998/  5000 (80%)]\tLearning rate: 0.0001\tLoss: 0.165975\n",
      "Train Epoch:  1 [  4198/  5000 (84%)]\tLearning rate: 0.0001\tLoss: 0.162362\n",
      "Train Epoch:  1 [  4398/  5000 (88%)]\tLearning rate: 0.0001\tLoss: 0.207034\n",
      "Train Epoch:  1 [  4598/  5000 (92%)]\tLearning rate: 0.0001\tLoss: 0.196116\n",
      "Train Epoch:  1 [  4798/  5000 (96%)]\tLearning rate: 0.0001\tLoss: 0.179708\n",
      "Train Epoch:  1 [  4998/  5000 (100%)]\tLearning rate: 0.0001\tLoss: 0.173016\n",
      "\n",
      "Validation set: Average loss: 0.164808\n",
      "\n",
      "Train Epoch:  2 [   198/  5000 (4%)]\tLearning rate: 0.0001\tLoss: 0.166549\n",
      "Train Epoch:  2 [   398/  5000 (8%)]\tLearning rate: 0.0001\tLoss: 0.169399\n",
      "Train Epoch:  2 [   598/  5000 (12%)]\tLearning rate: 0.0001\tLoss: 0.176245\n",
      "Train Epoch:  2 [   798/  5000 (16%)]\tLearning rate: 0.0001\tLoss: 0.186488\n",
      "Train Epoch:  2 [   998/  5000 (20%)]\tLearning rate: 0.0001\tLoss: 0.164256\n",
      "Train Epoch:  2 [  1198/  5000 (24%)]\tLearning rate: 0.0001\tLoss: 0.171506\n",
      "Train Epoch:  2 [  1398/  5000 (28%)]\tLearning rate: 0.0001\tLoss: 0.198275\n",
      "Train Epoch:  2 [  1598/  5000 (32%)]\tLearning rate: 0.0001\tLoss: 0.176877\n",
      "Train Epoch:  2 [  1798/  5000 (36%)]\tLearning rate: 0.0001\tLoss: 0.158574\n",
      "Train Epoch:  2 [  1998/  5000 (40%)]\tLearning rate: 0.0001\tLoss: 0.162765\n",
      "Train Epoch:  2 [  2198/  5000 (44%)]\tLearning rate: 0.0001\tLoss: 0.194720\n",
      "Train Epoch:  2 [  2398/  5000 (48%)]\tLearning rate: 0.0001\tLoss: 0.177608\n",
      "Train Epoch:  2 [  2598/  5000 (52%)]\tLearning rate: 0.0001\tLoss: 0.165258\n",
      "Train Epoch:  2 [  2798/  5000 (56%)]\tLearning rate: 0.0001\tLoss: 0.152012\n",
      "Train Epoch:  2 [  2998/  5000 (60%)]\tLearning rate: 0.0001\tLoss: 0.164559\n",
      "Train Epoch:  2 [  3198/  5000 (64%)]\tLearning rate: 0.0001\tLoss: 0.170932\n",
      "Train Epoch:  2 [  3398/  5000 (68%)]\tLearning rate: 0.0001\tLoss: 0.167526\n",
      "Train Epoch:  2 [  3598/  5000 (72%)]\tLearning rate: 0.0001\tLoss: 0.147793\n",
      "Train Epoch:  2 [  3798/  5000 (76%)]\tLearning rate: 0.0001\tLoss: 0.130615\n",
      "Train Epoch:  2 [  3998/  5000 (80%)]\tLearning rate: 0.0001\tLoss: 0.165241\n",
      "Train Epoch:  2 [  4198/  5000 (84%)]\tLearning rate: 0.0001\tLoss: 0.161379\n",
      "Train Epoch:  2 [  4398/  5000 (88%)]\tLearning rate: 0.0001\tLoss: 0.205014\n",
      "Train Epoch:  2 [  4598/  5000 (92%)]\tLearning rate: 0.0001\tLoss: 0.194731\n",
      "Train Epoch:  2 [  4798/  5000 (96%)]\tLearning rate: 0.0001\tLoss: 0.178071\n",
      "Train Epoch:  2 [  4998/  5000 (100%)]\tLearning rate: 0.0001\tLoss: 0.171837\n",
      "\n",
      "Validation set: Average loss: 0.164075\n",
      "\n",
      "Train Epoch:  3 [   198/  5000 (4%)]\tLearning rate: 0.0001\tLoss: 0.165604\n",
      "Train Epoch:  3 [   398/  5000 (8%)]\tLearning rate: 0.0001\tLoss: 0.168083\n",
      "Train Epoch:  3 [   598/  5000 (12%)]\tLearning rate: 0.0001\tLoss: 0.175658\n",
      "Train Epoch:  3 [   798/  5000 (16%)]\tLearning rate: 0.0001\tLoss: 0.185459\n",
      "Train Epoch:  3 [   998/  5000 (20%)]\tLearning rate: 0.0001\tLoss: 0.163022\n",
      "Train Epoch:  3 [  1198/  5000 (24%)]\tLearning rate: 0.0001\tLoss: 0.170243\n",
      "Train Epoch:  3 [  1398/  5000 (28%)]\tLearning rate: 0.0001\tLoss: 0.196833\n",
      "Train Epoch:  3 [  1598/  5000 (32%)]\tLearning rate: 0.0001\tLoss: 0.176141\n",
      "Train Epoch:  3 [  1798/  5000 (36%)]\tLearning rate: 0.0001\tLoss: 0.157592\n",
      "Train Epoch:  3 [  1998/  5000 (40%)]\tLearning rate: 0.0001\tLoss: 0.161802\n",
      "Train Epoch:  3 [  2198/  5000 (44%)]\tLearning rate: 0.0001\tLoss: 0.193528\n",
      "Train Epoch:  3 [  2398/  5000 (48%)]\tLearning rate: 0.0001\tLoss: 0.176688\n",
      "Train Epoch:  3 [  2598/  5000 (52%)]\tLearning rate: 0.0001\tLoss: 0.164138\n",
      "Train Epoch:  3 [  2798/  5000 (56%)]\tLearning rate: 0.0001\tLoss: 0.151373\n",
      "Train Epoch:  3 [  2998/  5000 (60%)]\tLearning rate: 0.0001\tLoss: 0.163339\n",
      "Train Epoch:  3 [  3198/  5000 (64%)]\tLearning rate: 0.0001\tLoss: 0.170028\n",
      "Train Epoch:  3 [  3398/  5000 (68%)]\tLearning rate: 0.0001\tLoss: 0.166797\n",
      "Train Epoch:  3 [  3598/  5000 (72%)]\tLearning rate: 0.0001\tLoss: 0.146957\n",
      "Train Epoch:  3 [  3798/  5000 (76%)]\tLearning rate: 0.0001\tLoss: 0.129732\n",
      "Train Epoch:  3 [  3998/  5000 (80%)]\tLearning rate: 0.0001\tLoss: 0.164546\n",
      "Train Epoch:  3 [  4198/  5000 (84%)]\tLearning rate: 0.0001\tLoss: 0.160697\n",
      "Train Epoch:  3 [  4398/  5000 (88%)]\tLearning rate: 0.0001\tLoss: 0.203629\n",
      "Train Epoch:  3 [  4598/  5000 (92%)]\tLearning rate: 0.0001\tLoss: 0.193712\n",
      "Train Epoch:  3 [  4798/  5000 (96%)]\tLearning rate: 0.0001\tLoss: 0.176641\n",
      "Train Epoch:  3 [  4998/  5000 (100%)]\tLearning rate: 0.0001\tLoss: 0.170656\n",
      "\n",
      "Validation set: Average loss: 0.163268\n",
      "\n",
      "Train Epoch:  4 [   198/  5000 (4%)]\tLearning rate: 0.0001\tLoss: 0.164758\n",
      "Train Epoch:  4 [   398/  5000 (8%)]\tLearning rate: 0.0001\tLoss: 0.166863\n",
      "Train Epoch:  4 [   598/  5000 (12%)]\tLearning rate: 0.0001\tLoss: 0.174902\n",
      "Train Epoch:  4 [   798/  5000 (16%)]\tLearning rate: 0.0001\tLoss: 0.184564\n",
      "Train Epoch:  4 [   998/  5000 (20%)]\tLearning rate: 0.0001\tLoss: 0.161908\n",
      "Train Epoch:  4 [  1198/  5000 (24%)]\tLearning rate: 0.0001\tLoss: 0.169194\n",
      "Train Epoch:  4 [  1398/  5000 (28%)]\tLearning rate: 0.0001\tLoss: 0.195446\n",
      "Train Epoch:  4 [  1598/  5000 (32%)]\tLearning rate: 0.0001\tLoss: 0.175349\n",
      "Train Epoch:  4 [  1798/  5000 (36%)]\tLearning rate: 0.0001\tLoss: 0.156613\n",
      "Train Epoch:  4 [  1998/  5000 (40%)]\tLearning rate: 0.0001\tLoss: 0.160728\n",
      "Train Epoch:  4 [  2198/  5000 (44%)]\tLearning rate: 0.0001\tLoss: 0.192303\n",
      "Train Epoch:  4 [  2398/  5000 (48%)]\tLearning rate: 0.0001\tLoss: 0.175612\n",
      "Train Epoch:  4 [  2598/  5000 (52%)]\tLearning rate: 0.0001\tLoss: 0.163006\n",
      "Train Epoch:  4 [  2798/  5000 (56%)]\tLearning rate: 0.0001\tLoss: 0.150888\n",
      "Train Epoch:  4 [  2998/  5000 (60%)]\tLearning rate: 0.0001\tLoss: 0.162095\n",
      "Train Epoch:  4 [  3198/  5000 (64%)]\tLearning rate: 0.0001\tLoss: 0.168975\n",
      "Train Epoch:  4 [  3398/  5000 (68%)]\tLearning rate: 0.0001\tLoss: 0.165894\n",
      "Train Epoch:  4 [  3598/  5000 (72%)]\tLearning rate: 0.0001\tLoss: 0.145973\n",
      "Train Epoch:  4 [  3798/  5000 (76%)]\tLearning rate: 0.0001\tLoss: 0.128912\n",
      "Train Epoch:  4 [  3998/  5000 (80%)]\tLearning rate: 0.0001\tLoss: 0.163690\n",
      "Train Epoch:  4 [  4198/  5000 (84%)]\tLearning rate: 0.0001\tLoss: 0.159784\n",
      "Train Epoch:  4 [  4398/  5000 (88%)]\tLearning rate: 0.0001\tLoss: 0.202211\n",
      "Train Epoch:  4 [  4598/  5000 (92%)]\tLearning rate: 0.0001\tLoss: 0.192628\n",
      "Train Epoch:  4 [  4798/  5000 (96%)]\tLearning rate: 0.0001\tLoss: 0.175103\n",
      "Train Epoch:  4 [  4998/  5000 (100%)]\tLearning rate: 0.0001\tLoss: 0.169324\n",
      "\n",
      "Validation set: Average loss: 0.162272\n",
      "\n",
      "Train Epoch:  5 [   198/  5000 (4%)]\tLearning rate: 0.0001\tLoss: 0.163710\n",
      "Train Epoch:  5 [   398/  5000 (8%)]\tLearning rate: 0.0001\tLoss: 0.165471\n",
      "Train Epoch:  5 [   598/  5000 (12%)]\tLearning rate: 0.0001\tLoss: 0.174100\n",
      "Train Epoch:  5 [   798/  5000 (16%)]\tLearning rate: 0.0001\tLoss: 0.183477\n",
      "Train Epoch:  5 [   998/  5000 (20%)]\tLearning rate: 0.0001\tLoss: 0.160678\n",
      "Train Epoch:  5 [  1198/  5000 (24%)]\tLearning rate: 0.0001\tLoss: 0.168022\n",
      "Train Epoch:  5 [  1398/  5000 (28%)]\tLearning rate: 0.0001\tLoss: 0.193621\n",
      "Train Epoch:  5 [  1598/  5000 (32%)]\tLearning rate: 0.0001\tLoss: 0.174208\n",
      "Train Epoch:  5 [  1798/  5000 (36%)]\tLearning rate: 0.0001\tLoss: 0.155371\n",
      "Train Epoch:  5 [  1998/  5000 (40%)]\tLearning rate: 0.0001\tLoss: 0.159337\n",
      "Train Epoch:  5 [  2198/  5000 (44%)]\tLearning rate: 0.0001\tLoss: 0.190806\n",
      "Train Epoch:  5 [  2398/  5000 (48%)]\tLearning rate: 0.0001\tLoss: 0.174202\n",
      "Train Epoch:  5 [  2598/  5000 (52%)]\tLearning rate: 0.0001\tLoss: 0.161722\n",
      "Train Epoch:  5 [  2798/  5000 (56%)]\tLearning rate: 0.0001\tLoss: 0.150327\n",
      "Train Epoch:  5 [  2998/  5000 (60%)]\tLearning rate: 0.0001\tLoss: 0.160665\n",
      "Train Epoch:  5 [  3198/  5000 (64%)]\tLearning rate: 0.0001\tLoss: 0.167622\n",
      "Train Epoch:  5 [  3398/  5000 (68%)]\tLearning rate: 0.0001\tLoss: 0.164763\n",
      "Train Epoch:  5 [  3598/  5000 (72%)]\tLearning rate: 0.0001\tLoss: 0.144802\n",
      "Train Epoch:  5 [  3798/  5000 (76%)]\tLearning rate: 0.0001\tLoss: 0.127786\n",
      "Train Epoch:  5 [  3998/  5000 (80%)]\tLearning rate: 0.0001\tLoss: 0.162595\n",
      "Train Epoch:  5 [  4198/  5000 (84%)]\tLearning rate: 0.0001\tLoss: 0.158553\n",
      "Train Epoch:  5 [  4398/  5000 (88%)]\tLearning rate: 0.0001\tLoss: 0.200588\n",
      "Train Epoch:  5 [  4598/  5000 (92%)]\tLearning rate: 0.0001\tLoss: 0.191109\n",
      "Train Epoch:  5 [  4798/  5000 (96%)]\tLearning rate: 0.0001\tLoss: 0.173233\n",
      "Train Epoch:  5 [  4998/  5000 (100%)]\tLearning rate: 0.0001\tLoss: 0.167737\n",
      "\n",
      "Validation set: Average loss: 0.160816\n",
      "\n",
      "Train Epoch:  6 [   198/  5000 (4%)]\tLearning rate: 0.0001\tLoss: 0.162557\n",
      "Train Epoch:  6 [   398/  5000 (8%)]\tLearning rate: 0.0001\tLoss: 0.163975\n",
      "Train Epoch:  6 [   598/  5000 (12%)]\tLearning rate: 0.0001\tLoss: 0.172849\n",
      "Train Epoch:  6 [   798/  5000 (16%)]\tLearning rate: 0.0001\tLoss: 0.181999\n",
      "Train Epoch:  6 [   998/  5000 (20%)]\tLearning rate: 0.0001\tLoss: 0.159069\n",
      "Train Epoch:  6 [  1198/  5000 (24%)]\tLearning rate: 0.0001\tLoss: 0.166416\n",
      "Train Epoch:  6 [  1398/  5000 (28%)]\tLearning rate: 0.0001\tLoss: 0.191340\n",
      "Train Epoch:  6 [  1598/  5000 (32%)]\tLearning rate: 0.0001\tLoss: 0.172648\n",
      "Train Epoch:  6 [  1798/  5000 (36%)]\tLearning rate: 0.0001\tLoss: 0.153681\n",
      "Train Epoch:  6 [  1998/  5000 (40%)]\tLearning rate: 0.0001\tLoss: 0.157391\n",
      "Train Epoch:  6 [  2198/  5000 (44%)]\tLearning rate: 0.0001\tLoss: 0.188933\n",
      "Train Epoch:  6 [  2398/  5000 (48%)]\tLearning rate: 0.0001\tLoss: 0.172213\n",
      "Train Epoch:  6 [  2598/  5000 (52%)]\tLearning rate: 0.0001\tLoss: 0.160112\n",
      "Train Epoch:  6 [  2798/  5000 (56%)]\tLearning rate: 0.0001\tLoss: 0.149805\n",
      "Train Epoch:  6 [  2998/  5000 (60%)]\tLearning rate: 0.0001\tLoss: 0.158778\n",
      "Train Epoch:  6 [  3198/  5000 (64%)]\tLearning rate: 0.0001\tLoss: 0.165813\n",
      "Train Epoch:  6 [  3398/  5000 (68%)]\tLearning rate: 0.0001\tLoss: 0.163335\n",
      "Train Epoch:  6 [  3598/  5000 (72%)]\tLearning rate: 0.0001\tLoss: 0.143278\n",
      "Train Epoch:  6 [  3798/  5000 (76%)]\tLearning rate: 0.0001\tLoss: 0.126317\n",
      "Train Epoch:  6 [  3998/  5000 (80%)]\tLearning rate: 0.0001\tLoss: 0.161014\n",
      "Train Epoch:  6 [  4198/  5000 (84%)]\tLearning rate: 0.0001\tLoss: 0.157002\n",
      "Train Epoch:  6 [  4398/  5000 (88%)]\tLearning rate: 0.0001\tLoss: 0.198428\n",
      "Train Epoch:  6 [  4598/  5000 (92%)]\tLearning rate: 0.0001\tLoss: 0.189107\n",
      "Train Epoch:  6 [  4798/  5000 (96%)]\tLearning rate: 0.0001\tLoss: 0.170956\n",
      "Train Epoch:  6 [  4998/  5000 (100%)]\tLearning rate: 0.0001\tLoss: 0.165569\n",
      "\n",
      "Validation set: Average loss: 0.158654\n",
      "\n",
      "Train Epoch:  7 [   198/  5000 (4%)]\tLearning rate: 0.0001\tLoss: 0.160730\n",
      "Train Epoch:  7 [   398/  5000 (8%)]\tLearning rate: 0.0001\tLoss: 0.162011\n",
      "Train Epoch:  7 [   598/  5000 (12%)]\tLearning rate: 0.0001\tLoss: 0.171182\n",
      "Train Epoch:  7 [   798/  5000 (16%)]\tLearning rate: 0.0001\tLoss: 0.179848\n",
      "Train Epoch:  7 [   998/  5000 (20%)]\tLearning rate: 0.0001\tLoss: 0.156831\n",
      "Train Epoch:  7 [  1198/  5000 (24%)]\tLearning rate: 0.0001\tLoss: 0.164430\n",
      "Train Epoch:  7 [  1398/  5000 (28%)]\tLearning rate: 0.0001\tLoss: 0.188440\n",
      "Train Epoch:  7 [  1598/  5000 (32%)]\tLearning rate: 0.0001\tLoss: 0.170428\n",
      "Train Epoch:  7 [  1798/  5000 (36%)]\tLearning rate: 0.0001\tLoss: 0.151470\n",
      "Train Epoch:  7 [  1998/  5000 (40%)]\tLearning rate: 0.0001\tLoss: 0.154840\n",
      "Train Epoch:  7 [  2198/  5000 (44%)]\tLearning rate: 0.0001\tLoss: 0.186572\n",
      "Train Epoch:  7 [  2398/  5000 (48%)]\tLearning rate: 0.0001\tLoss: 0.169220\n",
      "Train Epoch:  7 [  2598/  5000 (52%)]\tLearning rate: 0.0001\tLoss: 0.157813\n",
      "Train Epoch:  7 [  2798/  5000 (56%)]\tLearning rate: 0.0001\tLoss: 0.148826\n",
      "Train Epoch:  7 [  2998/  5000 (60%)]\tLearning rate: 0.0001\tLoss: 0.156013\n",
      "Train Epoch:  7 [  3198/  5000 (64%)]\tLearning rate: 0.0001\tLoss: 0.162690\n",
      "Train Epoch:  7 [  3398/  5000 (68%)]\tLearning rate: 0.0001\tLoss: 0.160732\n",
      "Train Epoch:  7 [  3598/  5000 (72%)]\tLearning rate: 0.0001\tLoss: 0.141278\n",
      "Train Epoch:  7 [  3798/  5000 (76%)]\tLearning rate: 0.0001\tLoss: 0.124103\n",
      "Train Epoch:  7 [  3998/  5000 (80%)]\tLearning rate: 0.0001\tLoss: 0.158554\n",
      "Train Epoch:  7 [  4198/  5000 (84%)]\tLearning rate: 0.0001\tLoss: 0.154701\n",
      "Train Epoch:  7 [  4398/  5000 (88%)]\tLearning rate: 0.0001\tLoss: 0.195467\n",
      "Train Epoch:  7 [  4598/  5000 (92%)]\tLearning rate: 0.0001\tLoss: 0.185806\n",
      "Train Epoch:  7 [  4798/  5000 (96%)]\tLearning rate: 0.0001\tLoss: 0.167819\n",
      "Train Epoch:  7 [  4998/  5000 (100%)]\tLearning rate: 0.0001\tLoss: 0.162276\n",
      "\n",
      "Validation set: Average loss: 0.155341\n",
      "\n",
      "Train Epoch:  8 [   198/  5000 (4%)]\tLearning rate: 0.0001\tLoss: 0.157643\n",
      "Train Epoch:  8 [   398/  5000 (8%)]\tLearning rate: 0.0001\tLoss: 0.159020\n",
      "Train Epoch:  8 [   598/  5000 (12%)]\tLearning rate: 0.0001\tLoss: 0.168375\n",
      "Train Epoch:  8 [   798/  5000 (16%)]\tLearning rate: 0.0001\tLoss: 0.176356\n",
      "Train Epoch:  8 [   998/  5000 (20%)]\tLearning rate: 0.0001\tLoss: 0.153564\n",
      "Train Epoch:  8 [  1198/  5000 (24%)]\tLearning rate: 0.0001\tLoss: 0.161547\n",
      "Train Epoch:  8 [  1398/  5000 (28%)]\tLearning rate: 0.0001\tLoss: 0.183748\n",
      "Train Epoch:  8 [  1598/  5000 (32%)]\tLearning rate: 0.0001\tLoss: 0.166417\n",
      "Train Epoch:  8 [  1798/  5000 (36%)]\tLearning rate: 0.0001\tLoss: 0.148113\n",
      "Train Epoch:  8 [  1998/  5000 (40%)]\tLearning rate: 0.0001\tLoss: 0.150772\n",
      "Train Epoch:  8 [  2198/  5000 (44%)]\tLearning rate: 0.0001\tLoss: 0.182380\n",
      "Train Epoch:  8 [  2398/  5000 (48%)]\tLearning rate: 0.0001\tLoss: 0.164565\n",
      "Train Epoch:  8 [  2598/  5000 (52%)]\tLearning rate: 0.0001\tLoss: 0.153851\n",
      "Train Epoch:  8 [  2798/  5000 (56%)]\tLearning rate: 0.0001\tLoss: 0.147333\n",
      "Train Epoch:  8 [  2998/  5000 (60%)]\tLearning rate: 0.0001\tLoss: 0.151790\n",
      "Train Epoch:  8 [  3198/  5000 (64%)]\tLearning rate: 0.0001\tLoss: 0.157908\n",
      "Train Epoch:  8 [  3398/  5000 (68%)]\tLearning rate: 0.0001\tLoss: 0.156667\n",
      "Train Epoch:  8 [  3598/  5000 (72%)]\tLearning rate: 0.0001\tLoss: 0.137980\n",
      "Train Epoch:  8 [  3798/  5000 (76%)]\tLearning rate: 0.0001\tLoss: 0.120808\n",
      "Train Epoch:  8 [  3998/  5000 (80%)]\tLearning rate: 0.0001\tLoss: 0.154338\n",
      "Train Epoch:  8 [  4198/  5000 (84%)]\tLearning rate: 0.0001\tLoss: 0.150863\n",
      "Train Epoch:  8 [  4398/  5000 (88%)]\tLearning rate: 0.0001\tLoss: 0.190635\n",
      "Train Epoch:  8 [  4598/  5000 (92%)]\tLearning rate: 0.0001\tLoss: 0.180217\n",
      "Train Epoch:  8 [  4798/  5000 (96%)]\tLearning rate: 0.0001\tLoss: 0.162606\n",
      "Train Epoch:  8 [  4998/  5000 (100%)]\tLearning rate: 0.0001\tLoss: 0.156976\n",
      "\n",
      "Validation set: Average loss: 0.149694\n",
      "\n",
      "Train Epoch:  9 [   198/  5000 (4%)]\tLearning rate: 0.0001\tLoss: 0.152844\n",
      "Train Epoch:  9 [   398/  5000 (8%)]\tLearning rate: 0.0001\tLoss: 0.154292\n",
      "Train Epoch:  9 [   598/  5000 (12%)]\tLearning rate: 0.0001\tLoss: 0.163599\n",
      "Train Epoch:  9 [   798/  5000 (16%)]\tLearning rate: 0.0001\tLoss: 0.170544\n",
      "Train Epoch:  9 [   998/  5000 (20%)]\tLearning rate: 0.0001\tLoss: 0.148155\n",
      "Train Epoch:  9 [  1198/  5000 (24%)]\tLearning rate: 0.0001\tLoss: 0.156604\n",
      "Train Epoch:  9 [  1398/  5000 (28%)]\tLearning rate: 0.0001\tLoss: 0.175619\n",
      "Train Epoch:  9 [  1598/  5000 (32%)]\tLearning rate: 0.0001\tLoss: 0.159724\n",
      "Train Epoch:  9 [  1798/  5000 (36%)]\tLearning rate: 0.0001\tLoss: 0.142133\n",
      "Train Epoch:  9 [  1998/  5000 (40%)]\tLearning rate: 0.0001\tLoss: 0.143828\n",
      "Train Epoch:  9 [  2198/  5000 (44%)]\tLearning rate: 0.0001\tLoss: 0.174811\n",
      "Train Epoch:  9 [  2398/  5000 (48%)]\tLearning rate: 0.0001\tLoss: 0.156252\n",
      "Train Epoch:  9 [  2598/  5000 (52%)]\tLearning rate: 0.0001\tLoss: 0.147102\n",
      "Train Epoch:  9 [  2798/  5000 (56%)]\tLearning rate: 0.0001\tLoss: 0.143479\n",
      "Train Epoch:  9 [  2998/  5000 (60%)]\tLearning rate: 0.0001\tLoss: 0.144060\n",
      "Train Epoch:  9 [  3198/  5000 (64%)]\tLearning rate: 0.0001\tLoss: 0.149051\n",
      "Train Epoch:  9 [  3398/  5000 (68%)]\tLearning rate: 0.0001\tLoss: 0.149278\n",
      "Train Epoch:  9 [  3598/  5000 (72%)]\tLearning rate: 0.0001\tLoss: 0.131747\n",
      "Train Epoch:  9 [  3798/  5000 (76%)]\tLearning rate: 0.0001\tLoss: 0.114924\n",
      "Train Epoch:  9 [  3998/  5000 (80%)]\tLearning rate: 0.0001\tLoss: 0.146041\n",
      "Train Epoch:  9 [  4198/  5000 (84%)]\tLearning rate: 0.0001\tLoss: 0.143562\n",
      "Train Epoch:  9 [  4398/  5000 (88%)]\tLearning rate: 0.0001\tLoss: 0.180888\n",
      "Train Epoch:  9 [  4598/  5000 (92%)]\tLearning rate: 0.0001\tLoss: 0.169480\n",
      "Train Epoch:  9 [  4798/  5000 (96%)]\tLearning rate: 0.0001\tLoss: 0.152539\n",
      "Train Epoch:  9 [  4998/  5000 (100%)]\tLearning rate: 0.0001\tLoss: 0.146522\n",
      "\n",
      "Validation set: Average loss: 0.139355\n",
      "\n",
      "Train Epoch: 10 [   198/  5000 (4%)]\tLearning rate: 0.0001\tLoss: 0.143123\n",
      "Train Epoch: 10 [   398/  5000 (8%)]\tLearning rate: 0.0001\tLoss: 0.144907\n",
      "Train Epoch: 10 [   598/  5000 (12%)]\tLearning rate: 0.0001\tLoss: 0.153606\n",
      "Train Epoch: 10 [   798/  5000 (16%)]\tLearning rate: 0.0001\tLoss: 0.157989\n",
      "Train Epoch: 10 [   998/  5000 (20%)]\tLearning rate: 0.0001\tLoss: 0.137165\n",
      "Train Epoch: 10 [  1198/  5000 (24%)]\tLearning rate: 0.0001\tLoss: 0.146123\n",
      "Train Epoch: 10 [  1398/  5000 (28%)]\tLearning rate: 0.0001\tLoss: 0.158190\n",
      "Train Epoch: 10 [  1598/  5000 (32%)]\tLearning rate: 0.0001\tLoss: 0.146366\n",
      "Train Epoch: 10 [  1798/  5000 (36%)]\tLearning rate: 0.0001\tLoss: 0.128777\n",
      "Train Epoch: 10 [  1998/  5000 (40%)]\tLearning rate: 0.0001\tLoss: 0.128650\n",
      "Train Epoch: 10 [  2198/  5000 (44%)]\tLearning rate: 0.0001\tLoss: 0.155936\n",
      "Train Epoch: 10 [  2398/  5000 (48%)]\tLearning rate: 0.0001\tLoss: 0.136502\n",
      "Train Epoch: 10 [  2598/  5000 (52%)]\tLearning rate: 0.0001\tLoss: 0.132407\n",
      "Train Epoch: 10 [  2798/  5000 (56%)]\tLearning rate: 0.0001\tLoss: 0.130049\n",
      "Train Epoch: 10 [  2998/  5000 (60%)]\tLearning rate: 0.0001\tLoss: 0.124388\n",
      "Train Epoch: 10 [  3198/  5000 (64%)]\tLearning rate: 0.0001\tLoss: 0.127845\n",
      "Train Epoch: 10 [  3398/  5000 (68%)]\tLearning rate: 0.0001\tLoss: 0.129998\n",
      "Train Epoch: 10 [  3598/  5000 (72%)]\tLearning rate: 0.0001\tLoss: 0.112597\n",
      "Train Epoch: 10 [  3798/  5000 (76%)]\tLearning rate: 0.0001\tLoss: 0.099571\n",
      "Train Epoch: 10 [  3998/  5000 (80%)]\tLearning rate: 0.0001\tLoss: 0.122270\n",
      "Train Epoch: 10 [  4198/  5000 (84%)]\tLearning rate: 0.0001\tLoss: 0.123566\n",
      "Train Epoch: 10 [  4398/  5000 (88%)]\tLearning rate: 0.0001\tLoss: 0.148295\n",
      "Train Epoch: 10 [  4598/  5000 (92%)]\tLearning rate: 0.0001\tLoss: 0.136784\n",
      "Train Epoch: 10 [  4798/  5000 (96%)]\tLearning rate: 0.0001\tLoss: 0.125153\n",
      "Train Epoch: 10 [  4998/  5000 (100%)]\tLearning rate: 0.0001\tLoss: 0.117097\n",
      "\n",
      "Validation set: Average loss: 0.125258\n",
      "\n",
      "Train Epoch: 11 [   198/  5000 (4%)]\tLearning rate: 0.0001\tLoss: 0.112899\n",
      "Train Epoch: 11 [   398/  5000 (8%)]\tLearning rate: 0.0001\tLoss: 0.113353\n",
      "Train Epoch: 11 [   598/  5000 (12%)]\tLearning rate: 0.0001\tLoss: 0.120844\n",
      "Train Epoch: 11 [   798/  5000 (16%)]\tLearning rate: 0.0001\tLoss: 0.114257\n",
      "Train Epoch: 11 [   998/  5000 (20%)]\tLearning rate: 0.0001\tLoss: 0.099352\n",
      "Train Epoch: 11 [  1198/  5000 (24%)]\tLearning rate: 0.0001\tLoss: 0.112759\n",
      "Train Epoch: 11 [  1398/  5000 (28%)]\tLearning rate: 0.0001\tLoss: 0.106271\n",
      "Train Epoch: 11 [  1598/  5000 (32%)]\tLearning rate: 0.0001\tLoss: 0.102466\n",
      "Train Epoch: 11 [  1798/  5000 (36%)]\tLearning rate: 0.0001\tLoss: 0.088154\n",
      "Train Epoch: 11 [  1998/  5000 (40%)]\tLearning rate: 0.0001\tLoss: 0.081984\n",
      "Train Epoch: 11 [  2198/  5000 (44%)]\tLearning rate: 0.0001\tLoss: 0.097164\n",
      "Train Epoch: 11 [  2398/  5000 (48%)]\tLearning rate: 0.0001\tLoss: 0.074041\n",
      "Train Epoch: 11 [  2598/  5000 (52%)]\tLearning rate: 0.0001\tLoss: 0.078842\n",
      "Train Epoch: 11 [  2798/  5000 (56%)]\tLearning rate: 0.0001\tLoss: 0.083336\n",
      "Train Epoch: 11 [  2998/  5000 (60%)]\tLearning rate: 0.0001\tLoss: 0.068471\n",
      "Train Epoch: 11 [  3198/  5000 (64%)]\tLearning rate: 0.0001\tLoss: 0.069327\n",
      "Train Epoch: 11 [  3398/  5000 (68%)]\tLearning rate: 0.0001\tLoss: 0.076567\n",
      "Train Epoch: 11 [  3598/  5000 (72%)]\tLearning rate: 0.0001\tLoss: 0.063348\n",
      "Train Epoch: 11 [  3798/  5000 (76%)]\tLearning rate: 0.0001\tLoss: 0.047495\n",
      "Train Epoch: 11 [  3998/  5000 (80%)]\tLearning rate: 0.0001\tLoss: 0.052804\n",
      "Train Epoch: 11 [  4198/  5000 (84%)]\tLearning rate: 0.0001\tLoss: 0.059515\n",
      "Train Epoch: 11 [  4398/  5000 (88%)]\tLearning rate: 0.0001\tLoss: 0.062057\n",
      "Train Epoch: 11 [  4598/  5000 (92%)]\tLearning rate: 0.0001\tLoss: 0.054717\n",
      "Train Epoch: 11 [  4798/  5000 (96%)]\tLearning rate: 0.0001\tLoss: 0.056766\n",
      "Train Epoch: 11 [  4998/  5000 (100%)]\tLearning rate: 0.0001\tLoss: 0.046136\n",
      "\n",
      "Validation set: Average loss: 0.066745\n",
      "\n",
      "Train Epoch: 12 [   198/  5000 (4%)]\tLearning rate: 0.0001\tLoss: 0.046355\n",
      "Train Epoch: 12 [   398/  5000 (8%)]\tLearning rate: 0.0001\tLoss: 0.040775\n",
      "Train Epoch: 12 [   598/  5000 (12%)]\tLearning rate: 0.0001\tLoss: 0.043814\n",
      "Train Epoch: 12 [   798/  5000 (16%)]\tLearning rate: 0.0001\tLoss: 0.037091\n",
      "Train Epoch: 12 [   998/  5000 (20%)]\tLearning rate: 0.0001\tLoss: 0.039322\n",
      "Train Epoch: 12 [  1198/  5000 (24%)]\tLearning rate: 0.0001\tLoss: 0.039684\n",
      "Train Epoch: 12 [  1398/  5000 (28%)]\tLearning rate: 0.0001\tLoss: 0.040297\n",
      "Train Epoch: 12 [  1598/  5000 (32%)]\tLearning rate: 0.0001\tLoss: 0.034926\n",
      "Train Epoch: 12 [  1798/  5000 (36%)]\tLearning rate: 0.0001\tLoss: 0.034999\n",
      "Train Epoch: 12 [  1998/  5000 (40%)]\tLearning rate: 0.0001\tLoss: 0.033415\n",
      "Train Epoch: 12 [  2198/  5000 (44%)]\tLearning rate: 0.0001\tLoss: 0.043288\n",
      "Train Epoch: 12 [  2398/  5000 (48%)]\tLearning rate: 0.0001\tLoss: 0.029487\n",
      "Train Epoch: 12 [  2598/  5000 (52%)]\tLearning rate: 0.0001\tLoss: 0.029755\n",
      "Train Epoch: 12 [  2798/  5000 (56%)]\tLearning rate: 0.0001\tLoss: 0.033053\n",
      "Train Epoch: 12 [  2998/  5000 (60%)]\tLearning rate: 0.0001\tLoss: 0.028202\n",
      "Train Epoch: 12 [  3198/  5000 (64%)]\tLearning rate: 0.0001\tLoss: 0.030822\n",
      "Train Epoch: 12 [  3398/  5000 (68%)]\tLearning rate: 0.0001\tLoss: 0.032060\n",
      "Train Epoch: 12 [  3598/  5000 (72%)]\tLearning rate: 0.0001\tLoss: 0.030922\n",
      "Train Epoch: 12 [  3798/  5000 (76%)]\tLearning rate: 0.0001\tLoss: 0.022536\n",
      "Train Epoch: 12 [  3998/  5000 (80%)]\tLearning rate: 0.0001\tLoss: 0.022263\n",
      "Train Epoch: 12 [  4198/  5000 (84%)]\tLearning rate: 0.0001\tLoss: 0.026063\n",
      "Train Epoch: 12 [  4398/  5000 (88%)]\tLearning rate: 0.0001\tLoss: 0.024948\n",
      "Train Epoch: 12 [  4598/  5000 (92%)]\tLearning rate: 0.0001\tLoss: 0.024205\n",
      "Train Epoch: 12 [  4798/  5000 (96%)]\tLearning rate: 0.0001\tLoss: 0.031572\n",
      "Train Epoch: 12 [  4998/  5000 (100%)]\tLearning rate: 0.0001\tLoss: 0.024573\n",
      "\n",
      "Validation set: Average loss: 0.033246\n",
      "\n",
      "Train Epoch: 13 [   198/  5000 (4%)]\tLearning rate: 0.0001\tLoss: 0.021840\n",
      "Train Epoch: 13 [   398/  5000 (8%)]\tLearning rate: 0.0001\tLoss: 0.019954\n",
      "Train Epoch: 13 [   598/  5000 (12%)]\tLearning rate: 0.0001\tLoss: 0.019187\n",
      "Train Epoch: 13 [   798/  5000 (16%)]\tLearning rate: 0.0001\tLoss: 0.019299\n",
      "Train Epoch: 13 [   998/  5000 (20%)]\tLearning rate: 0.0001\tLoss: 0.022512\n",
      "Train Epoch: 13 [  1198/  5000 (24%)]\tLearning rate: 0.0001\tLoss: 0.019353\n",
      "Train Epoch: 13 [  1398/  5000 (28%)]\tLearning rate: 0.0001\tLoss: 0.019311\n",
      "Train Epoch: 13 [  1598/  5000 (32%)]\tLearning rate: 0.0001\tLoss: 0.020544\n",
      "Train Epoch: 13 [  1798/  5000 (36%)]\tLearning rate: 0.0001\tLoss: 0.018363\n",
      "Train Epoch: 13 [  1998/  5000 (40%)]\tLearning rate: 0.0001\tLoss: 0.016619\n",
      "Train Epoch: 13 [  2198/  5000 (44%)]\tLearning rate: 0.0001\tLoss: 0.022100\n",
      "Train Epoch: 13 [  2398/  5000 (48%)]\tLearning rate: 0.0001\tLoss: 0.016553\n",
      "Train Epoch: 13 [  2598/  5000 (52%)]\tLearning rate: 0.0001\tLoss: 0.015775\n",
      "Train Epoch: 13 [  2798/  5000 (56%)]\tLearning rate: 0.0001\tLoss: 0.019656\n",
      "Train Epoch: 13 [  2998/  5000 (60%)]\tLearning rate: 0.0001\tLoss: 0.013709\n",
      "Train Epoch: 13 [  3198/  5000 (64%)]\tLearning rate: 0.0001\tLoss: 0.017218\n",
      "Train Epoch: 13 [  3398/  5000 (68%)]\tLearning rate: 0.0001\tLoss: 0.016745\n",
      "Train Epoch: 13 [  3598/  5000 (72%)]\tLearning rate: 0.0001\tLoss: 0.016176\n",
      "Train Epoch: 13 [  3798/  5000 (76%)]\tLearning rate: 0.0001\tLoss: 0.012718\n",
      "Train Epoch: 13 [  3998/  5000 (80%)]\tLearning rate: 0.0001\tLoss: 0.013024\n",
      "Train Epoch: 13 [  4198/  5000 (84%)]\tLearning rate: 0.0001\tLoss: 0.016668\n",
      "Train Epoch: 13 [  4398/  5000 (88%)]\tLearning rate: 0.0001\tLoss: 0.013560\n",
      "Train Epoch: 13 [  4598/  5000 (92%)]\tLearning rate: 0.0001\tLoss: 0.013403\n",
      "Train Epoch: 13 [  4798/  5000 (96%)]\tLearning rate: 0.0001\tLoss: 0.018352\n",
      "Train Epoch: 13 [  4998/  5000 (100%)]\tLearning rate: 0.0001\tLoss: 0.015756\n",
      "\n",
      "Validation set: Average loss: 0.016972\n",
      "\n",
      "Train Epoch: 14 [   198/  5000 (4%)]\tLearning rate: 0.0001\tLoss: 0.012673\n",
      "Train Epoch: 14 [   398/  5000 (8%)]\tLearning rate: 0.0001\tLoss: 0.013110\n",
      "Train Epoch: 14 [   598/  5000 (12%)]\tLearning rate: 0.0001\tLoss: 0.009976\n",
      "Train Epoch: 14 [   798/  5000 (16%)]\tLearning rate: 0.0001\tLoss: 0.012156\n",
      "Train Epoch: 14 [   998/  5000 (20%)]\tLearning rate: 0.0001\tLoss: 0.014072\n",
      "Train Epoch: 14 [  1198/  5000 (24%)]\tLearning rate: 0.0001\tLoss: 0.011107\n",
      "Train Epoch: 14 [  1398/  5000 (28%)]\tLearning rate: 0.0001\tLoss: 0.011739\n",
      "Train Epoch: 14 [  1598/  5000 (32%)]\tLearning rate: 0.0001\tLoss: 0.012853\n",
      "Train Epoch: 14 [  1798/  5000 (36%)]\tLearning rate: 0.0001\tLoss: 0.011005\n",
      "Train Epoch: 14 [  1998/  5000 (40%)]\tLearning rate: 0.0001\tLoss: 0.010120\n",
      "Train Epoch: 14 [  2198/  5000 (44%)]\tLearning rate: 0.0001\tLoss: 0.011449\n",
      "Train Epoch: 14 [  2398/  5000 (48%)]\tLearning rate: 0.0001\tLoss: 0.009150\n",
      "Train Epoch: 14 [  2598/  5000 (52%)]\tLearning rate: 0.0001\tLoss: 0.010632\n",
      "Train Epoch: 14 [  2798/  5000 (56%)]\tLearning rate: 0.0001\tLoss: 0.013165\n",
      "Train Epoch: 14 [  2998/  5000 (60%)]\tLearning rate: 0.0001\tLoss: 0.008572\n",
      "Train Epoch: 14 [  3198/  5000 (64%)]\tLearning rate: 0.0001\tLoss: 0.009922\n",
      "Train Epoch: 14 [  3398/  5000 (68%)]\tLearning rate: 0.0001\tLoss: 0.010712\n",
      "Train Epoch: 14 [  3598/  5000 (72%)]\tLearning rate: 0.0001\tLoss: 0.010479\n",
      "Train Epoch: 14 [  3798/  5000 (76%)]\tLearning rate: 0.0001\tLoss: 0.008702\n",
      "Train Epoch: 14 [  3998/  5000 (80%)]\tLearning rate: 0.0001\tLoss: 0.008527\n",
      "Train Epoch: 14 [  4198/  5000 (84%)]\tLearning rate: 0.0001\tLoss: 0.011085\n",
      "Train Epoch: 14 [  4398/  5000 (88%)]\tLearning rate: 0.0001\tLoss: 0.008521\n",
      "Train Epoch: 14 [  4598/  5000 (92%)]\tLearning rate: 0.0001\tLoss: 0.009581\n",
      "Train Epoch: 14 [  4798/  5000 (96%)]\tLearning rate: 0.0001\tLoss: 0.011505\n",
      "Train Epoch: 14 [  4998/  5000 (100%)]\tLearning rate: 0.0001\tLoss: 0.009680\n",
      "\n",
      "Validation set: Average loss: 0.009424\n",
      "\n",
      "Train Epoch: 15 [   198/  5000 (4%)]\tLearning rate: 0.0001\tLoss: 0.008475\n",
      "Train Epoch: 15 [   398/  5000 (8%)]\tLearning rate: 0.0001\tLoss: 0.009688\n",
      "Train Epoch: 15 [   598/  5000 (12%)]\tLearning rate: 0.0001\tLoss: 0.006444\n",
      "Train Epoch: 15 [   798/  5000 (16%)]\tLearning rate: 0.0001\tLoss: 0.008399\n",
      "Train Epoch: 15 [   998/  5000 (20%)]\tLearning rate: 0.0001\tLoss: 0.009915\n",
      "Train Epoch: 15 [  1198/  5000 (24%)]\tLearning rate: 0.0001\tLoss: 0.007370\n",
      "Train Epoch: 15 [  1398/  5000 (28%)]\tLearning rate: 0.0001\tLoss: 0.007767\n",
      "Train Epoch: 15 [  1598/  5000 (32%)]\tLearning rate: 0.0001\tLoss: 0.008670\n",
      "Train Epoch: 15 [  1798/  5000 (36%)]\tLearning rate: 0.0001\tLoss: 0.007889\n",
      "Train Epoch: 15 [  1998/  5000 (40%)]\tLearning rate: 0.0001\tLoss: 0.007312\n",
      "Train Epoch: 15 [  2198/  5000 (44%)]\tLearning rate: 0.0001\tLoss: 0.006974\n",
      "Train Epoch: 15 [  2398/  5000 (48%)]\tLearning rate: 0.0001\tLoss: 0.006223\n",
      "Train Epoch: 15 [  2598/  5000 (52%)]\tLearning rate: 0.0001\tLoss: 0.007984\n",
      "Train Epoch: 15 [  2798/  5000 (56%)]\tLearning rate: 0.0001\tLoss: 0.009598\n",
      "Train Epoch: 15 [  2998/  5000 (60%)]\tLearning rate: 0.0001\tLoss: 0.006020\n",
      "Train Epoch: 15 [  3198/  5000 (64%)]\tLearning rate: 0.0001\tLoss: 0.006956\n",
      "Train Epoch: 15 [  3398/  5000 (68%)]\tLearning rate: 0.0001\tLoss: 0.007253\n",
      "Train Epoch: 15 [  3598/  5000 (72%)]\tLearning rate: 0.0001\tLoss: 0.007805\n",
      "Train Epoch: 15 [  3798/  5000 (76%)]\tLearning rate: 0.0001\tLoss: 0.006145\n",
      "Train Epoch: 15 [  3998/  5000 (80%)]\tLearning rate: 0.0001\tLoss: 0.006282\n",
      "Train Epoch: 15 [  4198/  5000 (84%)]\tLearning rate: 0.0001\tLoss: 0.007717\n",
      "Train Epoch: 15 [  4398/  5000 (88%)]\tLearning rate: 0.0001\tLoss: 0.006464\n",
      "Train Epoch: 15 [  4598/  5000 (92%)]\tLearning rate: 0.0001\tLoss: 0.007005\n",
      "Train Epoch: 15 [  4798/  5000 (96%)]\tLearning rate: 0.0001\tLoss: 0.008168\n",
      "Train Epoch: 15 [  4998/  5000 (100%)]\tLearning rate: 0.0001\tLoss: 0.005901\n",
      "\n",
      "Validation set: Average loss: 0.009065\n",
      "\n",
      "Average Difference: 0.07622090578079224\n"
     ]
    }
   ],
   "source": [
    "class TemporalBlock(nn.Module):\n",
    "    def __init__(self, n_inputs, n_outputs, kernel_size, stride, dilation, padding, dropout=0.2):\n",
    "        super(TemporalBlock, self).__init__()\n",
    "        # Your Code:\n",
    "        self.conv1 = weight_norm(nn.Conv1d(n_inputs, n_outputs, kernel_size,\n",
    "                                           stride=stride, padding=padding, dilation=dilation))\n",
    "        self.chomp1 = Chomp1d(padding)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        \n",
    "        self.net = nn.Sequential(self.conv1, self.chomp1, self.relu1, self.dropout1,)\n",
    "        self.downsample = nn.Conv1d(n_inputs, n_outputs, 1) if n_inputs != n_outputs else None\n",
    "        self.relu = nn.ReLU()\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        self.conv1.weight.data.normal_(0, 0.01)\n",
    "        if self.downsample is not None:\n",
    "            self.downsample.weight.data.normal_(0, 0.01)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.net(x)\n",
    "        res = x if self.downsample is None else self.downsample(x)\n",
    "        return self.relu(out + res)\n",
    "\n",
    "\n",
    "# train the model with two conv layers\n",
    "torch.manual_seed(args.seed)\n",
    "if torch.cuda.is_available():\n",
    "    if not args.cuda:\n",
    "        print(\"WARNING: You have a CUDA device, so you should probably run with --cuda\")\n",
    "\n",
    "input_channels = 2\n",
    "n_classes = 1\n",
    "batch_size = args.batch_size\n",
    "seq_length = args.seq_len\n",
    "epochs = args.epochs\n",
    "\n",
    "\n",
    "# Note: We use a very simple setting here (assuming all levels have the same # of channels.\n",
    "channel_sizes = [args.nhid]*args.levels\n",
    "kernel_size = args.ksize\n",
    "dropout = args.dropout\n",
    "model = TCN(input_channels, n_classes, channel_sizes, kernel_size=kernel_size, dropout=dropout)\n",
    "\n",
    "if args.cuda:\n",
    "    model.cuda()\n",
    "    X_train = X_train.cuda()\n",
    "    Y_train = Y_train.cuda()\n",
    "    X_test = X_test.cuda()\n",
    "    Y_test = Y_test.cuda()\n",
    "\n",
    "lr = args.lr\n",
    "optimizer = getattr(optim, args.optim)(model.parameters(), lr=lr)\n",
    "\n",
    "\n",
    "for ep in range(1, epochs+1):\n",
    "    train(ep)\n",
    "    tloss = evaluate()\n",
    "\n",
    "# show the result\n",
    "preds = model(X_test)\n",
    "\n",
    "total_diff = 0\n",
    "for i,pred in enumerate(preds):\n",
    "    total_diff += np.abs(pred.data.item() - Y_test[i].item())\n",
    "\n",
    "\n",
    "print('Average Difference:', total_diff/len(Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kw7ddfgBieTO"
   },
   "source": [
    "**2(d)** Compare the result you get from 2(c) with the average difference you get from 2(b). Which network gives you a better result? Why that network gives a better result? Do you have any suggestions to improve the performance of the network that has poorer performance? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4i2zHicWjlLv"
   },
   "source": [
    "Your Answers:\n",
    "- The model with the 2 convolutional network resulted with an average difference of 0.0191, and the 1 convolutional network resulted with a average difference of 0.0762. This shows that the 2 convolutional network model performed noticeably better than the 1 layer one. \n",
    "- Adding layers in the convolutional network increases the complexity of and accuracy of the model, as relected from the results with, with the same parameters. \n",
    "- One way to improve the 1 layer model is to increase the total number of epochs, this allows the model to train longer for better accuracy. The learning rate can also be changed to increase the accuracy of the model. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BSTbPdLmiGFr"
   },
   "source": [
    "**Problem 3: EEGNet** In this question, we will use EEGNet, a compact convolutional network for EEG-based brain-computer interfaces. Your task is to process the given EEG data so that you can train and test the network on the dataset. For more detail about the EEGNet, please see the GitHub repo here:\n",
    "https://github.com/aliasvishnu/EEGNet.\n",
    "\n",
    "The EEG data given in this question are generated. Feel free to print or plot the data if you want to know what the dataset looks like. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {
    "id": "eliGBb404rAX"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m/Users/lawr_xd/Desktop/Columbia/BMENE4700/BMEN-E4700/BMEN4470_HW2_Fall_2023.ipynb Cell 32\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/lawr_xd/Desktop/Columbia/BMENE4700/BMEN-E4700/BMEN4470_HW2_Fall_2023.ipynb#X40sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# MOUNTING GOOGLE DRIVE WHERE DATA IS STORED\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/lawr_xd/Desktop/Columbia/BMENE4700/BMEN-E4700/BMEN4470_HW2_Fall_2023.ipynb#X40sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mgoogle\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcolab\u001b[39;00m \u001b[39mimport\u001b[39;00m drive\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/lawr_xd/Desktop/Columbia/BMENE4700/BMEN-E4700/BMEN4470_HW2_Fall_2023.ipynb#X40sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m drive\u001b[39m.\u001b[39mmount(\u001b[39m'\u001b[39m\u001b[39m/drive\u001b[39m\u001b[39m'\u001b[39m, force_remount\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'google'"
     ]
    }
   ],
   "source": [
    "# MOUNTING GOOGLE DRIVE WHERE DATA IS STORED\n",
    "from google.colab import drive\n",
    "drive.mount('/drive', force_remount=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {
    "id": "ixfh8Xpz42tU"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300, 120, 64) (1, 300)\n"
     ]
    }
   ],
   "source": [
    "# Import data and save as np.array\n",
    "Data = sio.loadmat('/Users/lawr_xd/Desktop/Columbia/BMENE4700/BMEN-E4700/Dataset/X.mat')\n",
    "Label = sio.loadmat('/Users/lawr_xd/Desktop/Columbia/BMENE4700/BMEN-E4700/Dataset/Y.mat')\n",
    "Dataset = Data['Data']\n",
    "Labels = Label['Labels']\n",
    "print(Dataset.shape, Labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {
    "id": "wOuwUDULsXxH"
   },
   "outputs": [],
   "source": [
    "# define the EEGNet\n",
    "class EEGNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EEGNet, self).__init__()\n",
    "        self.T = 120\n",
    "        \n",
    "        # Layer 1\n",
    "        self.conv1 = nn.Conv2d(1, 16, (1, 64), padding = 0)\n",
    "        self.batchnorm1 = nn.BatchNorm2d(16, False)\n",
    "        \n",
    "        # Layer 2\n",
    "        self.padding1 = nn.ZeroPad2d((16, 17, 0, 1))\n",
    "        self.conv2 = nn.Conv2d(1, 4, (2, 32))\n",
    "        self.batchnorm2 = nn.BatchNorm2d(4, False)\n",
    "        self.pooling2 = nn.MaxPool2d(2, 4)\n",
    "        \n",
    "        # Layer 3\n",
    "        self.padding2 = nn.ZeroPad2d((2, 1, 4, 3))\n",
    "        self.conv3 = nn.Conv2d(4, 4, (8, 4))\n",
    "        self.batchnorm3 = nn.BatchNorm2d(4, False)\n",
    "        self.pooling3 = nn.MaxPool2d((2, 4))\n",
    "\n",
    "        # FC Layer\n",
    "        # NOTE: This dimension will depend on the number of timestamps per sample in your data.\n",
    "        # here we have 120 timepoints\n",
    "        self.fc1 = nn.Linear(4*2*7, 1)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        # Layer 1\n",
    "        x = F.elu(self.conv1(x))\n",
    "        x = self.batchnorm1(x)\n",
    "        x = F.dropout(x, 0.25)\n",
    "        x = x.permute(0, 3, 1, 2)\n",
    "        \n",
    "        # Layer 2\n",
    "        x = self.padding1(x)\n",
    "        x = F.elu(self.conv2(x))\n",
    "        x = self.batchnorm2(x)\n",
    "        x = F.dropout(x, 0.25)\n",
    "        x = self.pooling2(x)\n",
    "        \n",
    "        # Layer 3\n",
    "        x = self.padding2(x)\n",
    "        x = F.elu(self.conv3(x))\n",
    "        x = self.batchnorm3(x)\n",
    "        x = F.dropout(x, 0.25)\n",
    "        x = self.pooling3(x)\n",
    "        \n",
    "        # FC Layer\n",
    "        x = x.reshape(-1, 4*2*7)\n",
    "        x = torch.sigmoid(self.fc1(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "net = EEGNet()\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(net.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {
    "id": "f5LisRGVsXoM"
   },
   "outputs": [],
   "source": [
    "# Define the evaluation function\n",
    "def evaluate(model, X, Y, params = [\"acc\"]):\n",
    "    results = []\n",
    "    batch_size = 50\n",
    "    \n",
    "    predicted = []\n",
    "    \n",
    "    for i in range(int(len(X)/batch_size)):\n",
    "        s = i*batch_size\n",
    "        e = i*batch_size+batch_size\n",
    "        \n",
    "        inputs = Variable(torch.from_numpy(X[s:e]))\n",
    "        pred = model(inputs)\n",
    "        \n",
    "        predicted.append(pred.data.cpu().numpy())\n",
    "        \n",
    "        \n",
    "    inputs = Variable(torch.from_numpy(X))\n",
    "    predicted = model(inputs)\n",
    "    \n",
    "    predicted = predicted.data.cpu().numpy()\n",
    "    \n",
    "    for param in params:\n",
    "        if param == 'acc':\n",
    "            results.append(accuracy_score(Y, np.round(predicted)))\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H9-_OKF_H4L3"
   },
   "source": [
    "**3(a)** Now, reshape the data so you can train it using EEGNet. To train the network, you also need to modify the network parameters to match the dimension of your training data. Please see the image [here](https://github.com/aliasvishnu/EEGNet/blob/master/EEGNet.png) for more information. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300, 1, 120, 64)\n"
     ]
    }
   ],
   "source": [
    "expX = np.expand_dims(Dataset, 0)\n",
    "swap_Data = np.moveaxis(expX, 0, 1)\n",
    "print(swap_Data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300,)\n"
     ]
    }
   ],
   "source": [
    "expY= np.reshape(Labels, 300)\n",
    "print(expY.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {
    "id": "W2VckhJizNsg"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300, 1, 120, 64) (300,)\n"
     ]
    }
   ],
   "source": [
    " # Your coode\n",
    "X = swap_Data\n",
    "Y = expY\n",
    "print(X.shape, Y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a0LaqrLuINkv"
   },
   "source": [
    "**3(b)** Now split the training, evaluation, and testing data. Please make sure you have about 80% for training, about 10% for evaluation, and about 10% for testing. Then train your network on the dataset and test it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {
    "id": "6_sj1h6usXcc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(240, 1, 120, 64) (30, 1, 120, 64) (30, 1, 120, 64)\n",
      "(240,) (30,) (30,)\n"
     ]
    }
   ],
   "source": [
    "# Your Code: \n",
    "# Please name the parameters as names given below in the print function.\n",
    "X_train = X[0:240]\n",
    "y_train = Y[0:240]\n",
    "\n",
    "X_val = X[240:270]\n",
    "y_val = Y[240:270]\n",
    "\n",
    "X_test = X[270:300]\n",
    "y_test = Y[270:300]\n",
    "\n",
    "print(X_train.shape, X_val.shape,X_test.shape)\n",
    "print(y_train.shape, y_val.shape, y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {
    "id": "ZHFpTZAxtXBl"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch  0\n",
      "Training Loss  tensor(0.8554)\n",
      "Train -  [0.9625]\n",
      "Validation -  [0.9666666666666667]\n",
      "\n",
      "Epoch  1\n",
      "Training Loss  tensor(1.2766)\n",
      "Train -  [0.9625]\n",
      "Validation -  [1.0]\n",
      "\n",
      "Epoch  2\n",
      "Training Loss  tensor(1.5767)\n",
      "Train -  [0.9916666666666667]\n",
      "Validation -  [1.0]\n",
      "\n",
      "Epoch  3\n",
      "Training Loss  tensor(0.8747)\n",
      "Train -  [0.9708333333333333]\n",
      "Validation -  [0.9666666666666667]\n",
      "\n",
      "Epoch  4\n",
      "Training Loss  tensor(1.1367)\n",
      "Train -  [0.9625]\n",
      "Validation -  [1.0]\n",
      "\n",
      "Epoch  5\n",
      "Training Loss  tensor(1.0704)\n",
      "Train -  [0.9583333333333334]\n",
      "Validation -  [0.9666666666666667]\n",
      "\n",
      "Epoch  6\n",
      "Training Loss  tensor(0.6153)\n",
      "Train -  [0.9625]\n",
      "Validation -  [1.0]\n",
      "\n",
      "Epoch  7\n",
      "Training Loss  tensor(0.6004)\n",
      "Train -  [0.9833333333333333]\n",
      "Validation -  [1.0]\n",
      "\n",
      "Epoch  8\n",
      "Training Loss  tensor(1.2305)\n",
      "Train -  [0.9875]\n",
      "Validation -  [1.0]\n",
      "\n",
      "Epoch  9\n",
      "Training Loss  tensor(0.7969)\n",
      "Train -  [0.9958333333333333]\n",
      "Validation -  [0.9333333333333333]\n",
      "\n",
      "Epoch  10\n",
      "Training Loss  tensor(0.9793)\n",
      "Train -  [0.9875]\n",
      "Validation -  [0.9666666666666667]\n",
      "\n",
      "Epoch  11\n",
      "Training Loss  tensor(1.1776)\n",
      "Train -  [0.9708333333333333]\n",
      "Validation -  [0.9666666666666667]\n",
      "\n",
      "Epoch  12\n",
      "Training Loss  tensor(0.6706)\n",
      "Train -  [0.9916666666666667]\n",
      "Validation -  [0.9666666666666667]\n",
      "\n",
      "Epoch  13\n",
      "Training Loss  tensor(1.0328)\n",
      "Train -  [0.9875]\n",
      "Validation -  [0.9333333333333333]\n",
      "\n",
      "Epoch  14\n",
      "Training Loss  tensor(1.0271)\n",
      "Train -  [0.9791666666666666]\n",
      "Validation -  [0.9666666666666667]\n",
      "\n",
      "Epoch  15\n",
      "Training Loss  tensor(0.5206)\n",
      "Train -  [0.9958333333333333]\n",
      "Validation -  [1.0]\n",
      "\n",
      "Epoch  16\n",
      "Training Loss  tensor(0.2198)\n",
      "Train -  [0.9666666666666667]\n",
      "Validation -  [1.0]\n",
      "\n",
      "Epoch  17\n",
      "Training Loss  tensor(0.4792)\n",
      "Train -  [0.9666666666666667]\n",
      "Validation -  [1.0]\n",
      "\n",
      "Epoch  18\n",
      "Training Loss  tensor(0.9696)\n",
      "Train -  [0.9708333333333333]\n",
      "Validation -  [0.9333333333333333]\n",
      "\n",
      "Epoch  19\n",
      "Training Loss  tensor(0.7105)\n",
      "Train -  [0.975]\n",
      "Validation -  [1.0]\n",
      "\n",
      "Epoch  20\n",
      "Training Loss  tensor(0.7594)\n",
      "Train -  [0.9666666666666667]\n",
      "Validation -  [1.0]\n",
      "\n",
      "Epoch  21\n",
      "Training Loss  tensor(0.4757)\n",
      "Train -  [0.9833333333333333]\n",
      "Validation -  [1.0]\n",
      "\n",
      "Epoch  22\n",
      "Training Loss  tensor(0.6081)\n",
      "Train -  [0.9916666666666667]\n",
      "Validation -  [0.9666666666666667]\n",
      "\n",
      "Epoch  23\n",
      "Training Loss  tensor(0.6120)\n",
      "Train -  [0.9791666666666666]\n",
      "Validation -  [0.9333333333333333]\n",
      "\n",
      "Epoch  24\n",
      "Training Loss  tensor(0.4731)\n",
      "Train -  [0.9708333333333333]\n",
      "Validation -  [0.9333333333333333]\n",
      "\n",
      "Epoch  25\n",
      "Training Loss  tensor(0.5968)\n",
      "Train -  [0.9958333333333333]\n",
      "Validation -  [1.0]\n",
      "\n",
      "Epoch  26\n",
      "Training Loss  tensor(0.8372)\n",
      "Train -  [0.9791666666666666]\n",
      "Validation -  [0.9333333333333333]\n",
      "\n",
      "Epoch  27\n",
      "Training Loss  tensor(0.6187)\n",
      "Train -  [0.975]\n",
      "Validation -  [1.0]\n",
      "\n",
      "Epoch  28\n",
      "Training Loss  tensor(0.7011)\n",
      "Train -  [0.9791666666666666]\n",
      "Validation -  [1.0]\n",
      "\n",
      "Epoch  29\n",
      "Training Loss  tensor(0.4805)\n",
      "Train -  [0.9875]\n",
      "Validation -  [1.0]\n",
      "\n",
      "Epoch  30\n",
      "Training Loss  tensor(0.3154)\n",
      "Train -  [0.9875]\n",
      "Validation -  [1.0]\n",
      "\n",
      "Epoch  31\n",
      "Training Loss  tensor(0.1510)\n",
      "Train -  [0.9916666666666667]\n",
      "Validation -  [0.9333333333333333]\n",
      "\n",
      "Epoch  32\n",
      "Training Loss  tensor(0.3864)\n",
      "Train -  [1.0]\n",
      "Validation -  [1.0]\n",
      "\n",
      "Epoch  33\n",
      "Training Loss  tensor(0.3621)\n",
      "Train -  [0.9875]\n",
      "Validation -  [1.0]\n",
      "\n",
      "Epoch  34\n",
      "Training Loss  tensor(1.3050)\n",
      "Train -  [0.9875]\n",
      "Validation -  [0.9666666666666667]\n",
      "\n",
      "Epoch  35\n",
      "Training Loss  tensor(0.3383)\n",
      "Train -  [0.975]\n",
      "Validation -  [0.9666666666666667]\n",
      "\n",
      "Epoch  36\n",
      "Training Loss  tensor(0.3954)\n",
      "Train -  [0.9875]\n",
      "Validation -  [0.9666666666666667]\n",
      "\n",
      "Epoch  37\n",
      "Training Loss  tensor(0.3743)\n",
      "Train -  [0.9875]\n",
      "Validation -  [1.0]\n",
      "\n",
      "Epoch  38\n",
      "Training Loss  tensor(0.5324)\n",
      "Train -  [0.975]\n",
      "Validation -  [0.9666666666666667]\n",
      "\n",
      "Epoch  39\n",
      "Training Loss  tensor(0.5291)\n",
      "Train -  [0.9875]\n",
      "Validation -  [0.9666666666666667]\n"
     ]
    }
   ],
   "source": [
    "# train and test on your dataset\n",
    "batch_size = 16\n",
    "\n",
    "for epoch in range(40):  # loop over the dataset multiple times\n",
    "    print(\"\\nEpoch \", epoch)\n",
    "    \n",
    "    running_loss = 0.0\n",
    "    for i in range(int(len(X_train)/batch_size-1)):\n",
    "        s = i*batch_size\n",
    "        e = i*batch_size+batch_size\n",
    "        \n",
    "        inputs = torch.from_numpy(X_train[s:e])\n",
    "        labels = torch.FloatTensor(np.array([y_train[s:e]]).T*1.0)\n",
    "        \n",
    "        # wrap them in Variable\n",
    "        inputs, labels = Variable(inputs), Variable(labels)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)   \n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        \n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.data\n",
    "    \n",
    "    # Validation accuracy\n",
    "    params = [\"acc\"]\n",
    "    print(\"Training Loss \", running_loss)\n",
    "    print(\"Train - \", evaluate(net, X_train, y_train, params))\n",
    "    print(\"Validation - \", evaluate(net, X_val, y_val, params))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {
    "id": "cOspEmnDmAcO"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test -  [0.7]\n"
     ]
    }
   ],
   "source": [
    "# print test accuracy\n",
    "print(\"Test - \", evaluate(net, X_test, y_test, params))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lkJ-O-yiI8KN"
   },
   "source": [
    "**3(c)** Now, split the data by 50% training, 40% validation, 10% testing, and retrain your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {
    "id": "ZtSpZfeVtWvu"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150, 1, 120, 64) (120, 1, 120, 64) (30, 1, 120, 64)\n",
      "(150,) (120,) (30,)\n"
     ]
    }
   ],
   "source": [
    "# Your Code:\n",
    "X_train = X[0:150]\n",
    "y_train = Y[0:150]\n",
    "\n",
    "X_val = X[150:270]\n",
    "y_val = Y[150:270]\n",
    "\n",
    "X_test = X[270:300]\n",
    "y_test = Y[270:300]\n",
    "\n",
    "print(X_train.shape, X_val.shape,X_test.shape)\n",
    "print(y_train.shape, y_val.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {
    "id": "9OAkh5W4o6T7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch  0\n",
      "['acc', 'auc', 'fmeasure']\n",
      "Training Loss  tensor(0.2622)\n",
      "Train -  [0.96]\n",
      "Validation -  [0.975]\n",
      "\n",
      "Epoch  1\n",
      "['acc', 'auc', 'fmeasure']\n",
      "Training Loss  tensor(0.2075)\n",
      "Train -  [0.9866666666666667]\n",
      "Validation -  [0.9916666666666667]\n",
      "\n",
      "Epoch  2\n",
      "['acc', 'auc', 'fmeasure']\n",
      "Training Loss  tensor(0.1915)\n",
      "Train -  [1.0]\n",
      "Validation -  [0.9916666666666667]\n",
      "\n",
      "Epoch  3\n",
      "['acc', 'auc', 'fmeasure']\n",
      "Training Loss  tensor(0.2229)\n",
      "Train -  [0.98]\n",
      "Validation -  [0.9916666666666667]\n",
      "\n",
      "Epoch  4\n",
      "['acc', 'auc', 'fmeasure']\n",
      "Training Loss  tensor(0.1260)\n",
      "Train -  [0.98]\n",
      "Validation -  [0.9916666666666667]\n",
      "\n",
      "Epoch  5\n",
      "['acc', 'auc', 'fmeasure']\n",
      "Training Loss  tensor(0.0961)\n",
      "Train -  [1.0]\n",
      "Validation -  [0.975]\n",
      "\n",
      "Epoch  6\n",
      "['acc', 'auc', 'fmeasure']\n",
      "Training Loss  tensor(0.1917)\n",
      "Train -  [1.0]\n",
      "Validation -  [0.9666666666666667]\n",
      "\n",
      "Epoch  7\n",
      "['acc', 'auc', 'fmeasure']\n",
      "Training Loss  tensor(0.3035)\n",
      "Train -  [0.9733333333333334]\n",
      "Validation -  [0.9666666666666667]\n",
      "\n",
      "Epoch  8\n",
      "['acc', 'auc', 'fmeasure']\n",
      "Training Loss  tensor(0.1697)\n",
      "Train -  [0.9933333333333333]\n",
      "Validation -  [1.0]\n",
      "\n",
      "Epoch  9\n",
      "['acc', 'auc', 'fmeasure']\n",
      "Training Loss  tensor(0.0972)\n",
      "Train -  [0.9866666666666667]\n",
      "Validation -  [0.9916666666666667]\n",
      "\n",
      "Epoch  10\n",
      "['acc', 'auc', 'fmeasure']\n",
      "Training Loss  tensor(0.6333)\n",
      "Train -  [0.9866666666666667]\n",
      "Validation -  [0.975]\n",
      "\n",
      "Epoch  11\n",
      "['acc', 'auc', 'fmeasure']\n",
      "Training Loss  tensor(0.3955)\n",
      "Train -  [1.0]\n",
      "Validation -  [0.95]\n",
      "\n",
      "Epoch  12\n",
      "['acc', 'auc', 'fmeasure']\n",
      "Training Loss  tensor(0.5819)\n",
      "Train -  [0.9933333333333333]\n",
      "Validation -  [0.9666666666666667]\n",
      "\n",
      "Epoch  13\n",
      "['acc', 'auc', 'fmeasure']\n",
      "Training Loss  tensor(0.0995)\n",
      "Train -  [0.9666666666666667]\n",
      "Validation -  [0.9583333333333334]\n",
      "\n",
      "Epoch  14\n",
      "['acc', 'auc', 'fmeasure']\n",
      "Training Loss  tensor(0.2415)\n",
      "Train -  [0.9866666666666667]\n",
      "Validation -  [0.975]\n",
      "\n",
      "Epoch  15\n",
      "['acc', 'auc', 'fmeasure']\n",
      "Training Loss  tensor(0.3975)\n",
      "Train -  [0.9733333333333334]\n",
      "Validation -  [0.9833333333333333]\n",
      "\n",
      "Epoch  16\n",
      "['acc', 'auc', 'fmeasure']\n",
      "Training Loss  tensor(0.1872)\n",
      "Train -  [0.9933333333333333]\n",
      "Validation -  [0.9833333333333333]\n",
      "\n",
      "Epoch  17\n",
      "['acc', 'auc', 'fmeasure']\n",
      "Training Loss  tensor(0.3470)\n",
      "Train -  [0.9866666666666667]\n",
      "Validation -  [0.9916666666666667]\n",
      "\n",
      "Epoch  18\n",
      "['acc', 'auc', 'fmeasure']\n",
      "Training Loss  tensor(0.1370)\n",
      "Train -  [0.9666666666666667]\n",
      "Validation -  [0.975]\n",
      "\n",
      "Epoch  19\n",
      "['acc', 'auc', 'fmeasure']\n",
      "Training Loss  tensor(0.0355)\n",
      "Train -  [0.9866666666666667]\n",
      "Validation -  [0.9916666666666667]\n",
      "\n",
      "Epoch  20\n",
      "['acc', 'auc', 'fmeasure']\n",
      "Training Loss  tensor(0.1152)\n",
      "Train -  [0.9866666666666667]\n",
      "Validation -  [1.0]\n",
      "\n",
      "Epoch  21\n",
      "['acc', 'auc', 'fmeasure']\n",
      "Training Loss  tensor(0.1893)\n",
      "Train -  [0.9866666666666667]\n",
      "Validation -  [0.9833333333333333]\n",
      "\n",
      "Epoch  22\n",
      "['acc', 'auc', 'fmeasure']\n",
      "Training Loss  tensor(0.2090)\n",
      "Train -  [0.9933333333333333]\n",
      "Validation -  [0.975]\n",
      "\n",
      "Epoch  23\n",
      "['acc', 'auc', 'fmeasure']\n",
      "Training Loss  tensor(0.4652)\n",
      "Train -  [0.9866666666666667]\n",
      "Validation -  [0.95]\n",
      "\n",
      "Epoch  24\n",
      "['acc', 'auc', 'fmeasure']\n",
      "Training Loss  tensor(0.4515)\n",
      "Train -  [0.9666666666666667]\n",
      "Validation -  [0.9833333333333333]\n",
      "\n",
      "Epoch  25\n",
      "['acc', 'auc', 'fmeasure']\n",
      "Training Loss  tensor(0.0555)\n",
      "Train -  [0.9933333333333333]\n",
      "Validation -  [0.975]\n",
      "\n",
      "Epoch  26\n",
      "['acc', 'auc', 'fmeasure']\n",
      "Training Loss  tensor(0.8285)\n",
      "Train -  [0.98]\n",
      "Validation -  [0.9916666666666667]\n",
      "\n",
      "Epoch  27\n",
      "['acc', 'auc', 'fmeasure']\n",
      "Training Loss  tensor(0.2240)\n",
      "Train -  [0.9866666666666667]\n",
      "Validation -  [0.9666666666666667]\n",
      "\n",
      "Epoch  28\n",
      "['acc', 'auc', 'fmeasure']\n",
      "Training Loss  tensor(0.2627)\n",
      "Train -  [0.9866666666666667]\n",
      "Validation -  [0.9583333333333334]\n",
      "\n",
      "Epoch  29\n",
      "['acc', 'auc', 'fmeasure']\n",
      "Training Loss  tensor(0.1378)\n",
      "Train -  [0.9733333333333334]\n",
      "Validation -  [0.9833333333333333]\n",
      "\n",
      "Epoch  30\n",
      "['acc', 'auc', 'fmeasure']\n",
      "Training Loss  tensor(0.1537)\n",
      "Train -  [0.9933333333333333]\n",
      "Validation -  [0.9833333333333333]\n",
      "\n",
      "Epoch  31\n",
      "['acc', 'auc', 'fmeasure']\n",
      "Training Loss  tensor(0.3153)\n",
      "Train -  [0.98]\n",
      "Validation -  [0.9583333333333334]\n",
      "\n",
      "Epoch  32\n",
      "['acc', 'auc', 'fmeasure']\n",
      "Training Loss  tensor(0.2824)\n",
      "Train -  [0.9866666666666667]\n",
      "Validation -  [0.9333333333333333]\n",
      "\n",
      "Epoch  33\n",
      "['acc', 'auc', 'fmeasure']\n",
      "Training Loss  tensor(0.3065)\n",
      "Train -  [0.98]\n",
      "Validation -  [0.9666666666666667]\n",
      "\n",
      "Epoch  34\n",
      "['acc', 'auc', 'fmeasure']\n",
      "Training Loss  tensor(0.2090)\n",
      "Train -  [0.9866666666666667]\n",
      "Validation -  [0.9666666666666667]\n",
      "\n",
      "Epoch  35\n",
      "['acc', 'auc', 'fmeasure']\n",
      "Training Loss  tensor(0.4488)\n",
      "Train -  [0.9933333333333333]\n",
      "Validation -  [0.9416666666666667]\n",
      "\n",
      "Epoch  36\n",
      "['acc', 'auc', 'fmeasure']\n",
      "Training Loss  tensor(0.1034)\n",
      "Train -  [0.9866666666666667]\n",
      "Validation -  [0.95]\n",
      "\n",
      "Epoch  37\n",
      "['acc', 'auc', 'fmeasure']\n",
      "Training Loss  tensor(0.1188)\n",
      "Train -  [0.98]\n",
      "Validation -  [0.9666666666666667]\n",
      "\n",
      "Epoch  38\n",
      "['acc', 'auc', 'fmeasure']\n",
      "Training Loss  tensor(0.1294)\n",
      "Train -  [0.9733333333333334]\n",
      "Validation -  [0.9916666666666667]\n",
      "\n",
      "Epoch  39\n",
      "['acc', 'auc', 'fmeasure']\n",
      "Training Loss  tensor(0.0603)\n",
      "Train -  [0.9933333333333333]\n",
      "Validation -  [0.95]\n"
     ]
    }
   ],
   "source": [
    "# train and test on the dataset with 50% training, 40% validation, and 10% test data\n",
    "batch_size = 16\n",
    "\n",
    "for epoch in range(40):  # loop over the dataset multiple times\n",
    "    print(\"\\nEpoch \", epoch)\n",
    "    \n",
    "    running_loss = 0.0\n",
    "    for i in range(int(len(X_train)/batch_size-1)):\n",
    "        s = i*batch_size\n",
    "        e = i*batch_size+batch_size\n",
    "        \n",
    "        inputs = torch.from_numpy(X_train[s:e])\n",
    "        labels = torch.FloatTensor(np.array([y_train[s:e]]).T*1.0)\n",
    "        \n",
    "        # wrap them in Variable\n",
    "        inputs, labels = Variable(inputs), Variable(labels)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)   \n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        \n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.data\n",
    "    \n",
    "    # Validation accuracy\n",
    "    params = [\"acc\", \"auc\", \"fmeasure\"]\n",
    "    print(params)\n",
    "    print(\"Training Loss \", running_loss)\n",
    "    print(\"Train - \", evaluate(net, X_train, y_train, params))\n",
    "    print(\"Validation - \", evaluate(net, X_val, y_val, params))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {
    "id": "oO_vOCcupGgV"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test -  [0.6666666666666666]\n"
     ]
    }
   ],
   "source": [
    "print(\"Test - \", evaluate(net, X_test, y_test, params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(240, 1, 120, 64) (30, 1, 120, 64) (30, 1, 120, 64)\n",
      "(240,) (30,) (30,)\n",
      "Test -  [0.9666666666666667]\n"
     ]
    }
   ],
   "source": [
    "#additional testing #80-10-10\n",
    "X_train = X[60:300]\n",
    "y_train = Y[60:300]\n",
    "\n",
    "X_val = X[0:30]\n",
    "y_val = Y[0:30]\n",
    "\n",
    "X_test = X[30:60]\n",
    "y_test = Y[30:60]\n",
    "\n",
    "print(X_train.shape, X_val.shape,X_test.shape)\n",
    "print(y_train.shape, y_val.shape, y_test.shape)\n",
    "\n",
    "batch_size = 16\n",
    "\n",
    "for epoch in range(40):  # loop over the dataset multiple times\n",
    "    #print(\"\\nEpoch \", epoch)\n",
    "    \n",
    "    running_loss = 0.0\n",
    "    for i in range(int(len(X_train)/batch_size-1)):\n",
    "        s = i*batch_size\n",
    "        e = i*batch_size+batch_size\n",
    "        \n",
    "        inputs = torch.from_numpy(X_train[s:e])\n",
    "        labels = torch.FloatTensor(np.array([y_train[s:e]]).T*1.0)\n",
    "        \n",
    "        # wrap them in Variable\n",
    "        inputs, labels = Variable(inputs), Variable(labels)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)   \n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        \n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.data\n",
    "    \n",
    "    # Validation accuracy\n",
    "    #params = [\"acc\"]\n",
    "    #print(\"Training Loss \", running_loss)\n",
    "    #print(\"Train - \", evaluate(net, X_train, y_train, params))\n",
    "    #print(\"Validation - \", evaluate(net, X_val, y_val, params))\n",
    "    \n",
    "\n",
    "print(\"Test - \", evaluate(net, X_test, y_test, params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150, 1, 120, 64) (120, 1, 120, 64) (30, 1, 120, 64)\n",
      "(150,) (120,) (30,)\n",
      "Test -  [1.0]\n"
     ]
    }
   ],
   "source": [
    "#additional testing #50-40-10\n",
    "X_train = X[150:300]\n",
    "y_train = Y[150:300]\n",
    "\n",
    "X_val = X[0:120]\n",
    "y_val = Y[0:120]\n",
    "\n",
    "X_test = X[120:150]\n",
    "y_test = Y[120:150]\n",
    "\n",
    "print(X_train.shape, X_val.shape,X_test.shape)\n",
    "print(y_train.shape, y_val.shape, y_test.shape)\n",
    "\n",
    "batch_size = 16\n",
    "\n",
    "for epoch in range(40):  # loop over the dataset multiple times\n",
    "    #print(\"\\nEpoch \", epoch)\n",
    "    \n",
    "    running_loss = 0.0\n",
    "    for i in range(int(len(X_train)/batch_size-1)):\n",
    "        s = i*batch_size\n",
    "        e = i*batch_size+batch_size\n",
    "        \n",
    "        inputs = torch.from_numpy(X_train[s:e])\n",
    "        labels = torch.FloatTensor(np.array([y_train[s:e]]).T*1.0)\n",
    "        \n",
    "        # wrap them in Variable\n",
    "        inputs, labels = Variable(inputs), Variable(labels)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)   \n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        \n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.data\n",
    "    \n",
    "    # Validation accuracy\n",
    "    #params = [\"acc\"]\n",
    "    #print(\"Training Loss \", running_loss)\n",
    "    #print(\"Train - \", evaluate(net, X_train, y_train, params))\n",
    "    #print(\"Validation - \", evaluate(net, X_val, y_val, params))\n",
    "    \n",
    "\n",
    "print(\"Test - \", evaluate(net, X_test, y_test, params))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uuLnQAkCs6OT"
   },
   "source": [
    "**3(d)** Compare the result you get from 3(d) with the test accuracy in 3(c). Which data split method gives you a better result? Why that method gives a better result? What did you learn from this?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0RGiDOnfvsyF"
   },
   "source": [
    "Your Answers:\n",
    "- Both split method gives comparable results. The 80-10-10 method "
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "BMEN4470_HW2_Fall_2021.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
